{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pyntcloud\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "from train import provider\n",
    "from train import box_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train.provider' from '/datasets/home/90/490/scgullap/ece_285_proj/frustum-pointnets/train/provider.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(box_util)\n",
    "importlib.reload(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINT=2048\n",
    "TRAIN_DATASET = provider.FrustumDataset(npoints=NUM_POINT, split='train',\\\n",
    "                            rotate_to_center=True, random_flip=False, random_shift=False, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATASET = provider.FrustumDataset(npoints=NUM_POINT, split='val',\\\n",
    "                            rotate_to_center=True, random_flip=False, random_shift=False, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_set,labels,c,d,e,f,g,h,i = TRAIN_DATASET[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73480"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.63603565,  1.47      ,  8.95986384],\n",
       "       [ 0.7288103 ,  1.47      ,  8.48891494],\n",
       "       [-0.44856195,  1.47      ,  8.25697831],\n",
       "       [-0.5413366 ,  1.47      ,  8.72792721],\n",
       "       [ 0.63603565, -0.42      ,  8.95986384],\n",
       "       [ 0.7288103 , -0.42      ,  8.48891494],\n",
       "       [-0.44856195, -0.42      ,  8.25697831],\n",
       "       [-0.5413366 , -0.42      ,  8.72792721]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATASET.get_center_view_box3d(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices(window_len, stride_len, point_set, labels, corners_gt):\n",
    "    IOU = []\n",
    "    start = []\n",
    "    end = []\n",
    "    IOU_1 = []\n",
    "    IOU_2 = []\n",
    "    corner = []\n",
    "    j = np.min(point_set,0)[2]\n",
    "    i = j+window_len\n",
    "    idtuple=set()\n",
    "    while i<=np.max(point_set,0)[2]:\n",
    "            max_ind = np.searchsorted(point_set[:,2], i)\n",
    "            min_ind = np.searchsorted(point_set[:,2], j)\n",
    "            if (min_ind,max_ind) not in idtuple and max_ind-min_ind>5:\n",
    "                #print('fgbd')\n",
    "                idtuple.add((min_ind,max_ind))\n",
    "                labels_window = np.zeros_like(point_set[:,0])\n",
    "                labels_window[min_ind:max_ind] = 1.0\n",
    "                union = np.sum(np.logical_or(labels, labels_window))\n",
    "                inter = np.sum(np.logical_and(labels, labels_window))\n",
    "                if (inter*1.0/union) < 0.3 or (inter*1.0/union) >0.7 :\n",
    "                    end.append(max_ind)\n",
    "                    start.append(min_ind)\n",
    "                    IOU.append(inter*1.0/union)\n",
    "            j = j+stride_len\n",
    "            i = j+window_len\n",
    "    IOU = np.array(IOU)>=0.7\n",
    "    return np.array(start),np.array(end), np.array(IOU)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points in each window:2048\n",
    "# number of windows\n",
    "#\n",
    "\n",
    "def get_list(start_idx,end_idx,num_points=1024,ensure_all=True):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    start_idx  : list of indexes of the starting point in each window\n",
    "    end_idx    : list of indees of ending point in each window\n",
    "    NOTES: we used two lists because we ran into issues with tensorflow when we used tuples,\n",
    "           this is a quick hack to avoid that\n",
    "    num_points : number of points to be sampled from i.e number of points in each window\n",
    "    ensure_all : in the case that there are fewwer than required points ensures all points are picked atleast once if selected as True\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    out:  an array of size \n",
    "          \n",
    "          num_windowsxnum_pointsx1\n",
    "          \n",
    "          this will be used inside tensorflow to divide into windows \n",
    "          after we pass the points trough pointnet\n",
    "    \"\"\"\n",
    "    \n",
    "    out= []\n",
    "    for i in range(np.shape(start_idx)[0]):\n",
    "        \n",
    "        if end_idx[i]-start_idx[i]+1<=num_points:\n",
    "            indexes=np.arange(start_idx[i],end_idx[i])\n",
    "            indexes2=np.random.choice(indexes, num_points-np.shape(indexes)[0],replace=True)\n",
    "            indexes=np.concatenate((indexes,indexes2))\n",
    "        else:\n",
    "            indexes=np.random.choice(np.arange(start_idx[i],end_idx[i]),num_points,replace=False)\n",
    "        out.append(np.reshape(indexes,(-1,1)))\n",
    "    \n",
    "    return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_util #for conv2d\n",
    "from model_util import NUM_HEADING_BIN, NUM_SIZE_CLUSTER, NUM_OBJECT_POINT\n",
    "from model_util import point_cloud_masking, get_center_regression_net\n",
    "from model_util import placeholder_inputs, parse_output_to_tensors, get_loss\n",
    "from model_util import huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "def focal_loss(sigmoid_p, target_tensor, weights=None, alpha=0.99, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log\n",
    "            (1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    #sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(target_tensor, dtype=target_tensor.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1.0 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classification_net(point_cloud, label_out_5, label_out_1, label_out_2, label_out_3, label_out_4, idx5, idx1, idx2, idx3, idx4, \n",
    "#                             is_training=True, bn_decay=True):\n",
    "def classification_net(point_cloud, label_out_5,  idx5, \n",
    "                            is_training=True, bn_decay=True):\n",
    "    '''\n",
    "    3D PointNet network.\n",
    "    param:\n",
    "    \n",
    "    point_cloud: TF tensor in shape (1,N,4)\n",
    "                 frustum point clouds with XYZ and intensity in point channels\n",
    "                 XYZs are in frustum coordinate\n",
    "    label_out: TF tensor in shape (1,1)\n",
    "            length-1 vector indicating the classification as car or background\n",
    "    \n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    \n",
    "    is_training: TF boolean scalar\n",
    "    bn_decay: TF float scalar\n",
    "    end_points: dict                 SHOULD CHANGE THIS\n",
    "    Output:                                         \n",
    "    logits: TF tensor in shape (B,N,2), scores for bkg/clutter and object\n",
    "    end_points: dict\n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    '''\n",
    "    \n",
    "    batch_size = point_cloud.get_shape()[0].value #1 in our case\n",
    "    num_point = point_cloud.get_shape()[1].value  #N in our case\n",
    "  \n",
    "    net = tf.expand_dims(point_cloud, 2)\n",
    "    #print(net.get_shape())\n",
    "\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    point_feat = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(point_feat, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    features = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "    \n",
    "    features=tf.squeeze(features,axis=2)\n",
    "    features=tf.squeeze(features,axis=0)   \n",
    "\n",
    "    #features=tf.placeholder(tf.float32,[1,1024,4])\n",
    "\n",
    "    def fn(x):\n",
    "        return tf.gather_nd(features,x)\n",
    "    \n",
    "    windows_5 =tf.map_fn(fn,idx5,dtype=tf.float32)\n",
    "#     windows_1 = tf.map_fn(fn,idx1,dtype=tf.float32)\n",
    "#     windows_2 = tf.map_fn(fn,idx2,dtype=tf.float32)\n",
    "#     windows_3 = tf.map_fn(fn,idx3,dtype=tf.float32)\n",
    "#     windows_4 = tf.map_fn(fn,idx4,dtype=tf.float32)\n",
    "    \n",
    "    windows_5_1024 = tf.reduce_max(windows_5,axis=1)\n",
    "#     windows_1_1024 = tf.reduce_max(windows_1,axis=1)\n",
    "#     windows_2_1024 = tf.reduce_max(windows_2,axis=1)\n",
    "#     windows_3_1024 = tf.reduce_max(windows_3,axis=1)\n",
    "#     windows_4_1024 = tf.reduce_max(windows_4,axis=1)\n",
    "    \n",
    "    \n",
    "    winex_5 =  tf.reshape(windows_5_1024,shape=[-1,1,1,1024])\n",
    "#     winex_1 =  tf.reshape(windows_1_1024,shape=[-1,1,1,1024])\n",
    "#     winex_2 =  tf.reshape(windows_2_1024,shape=[-1,1,1,1024])\n",
    "#     winex_3 =  tf.reshape(windows_3_1024,shape=[-1,1,1,1024])\n",
    "#     winex_4 =  tf.reshape(windows_4_1024,shape=[-1,1,1,1024])\n",
    "   \n",
    "    winex = tf.concat([winex_5], axis=0)\n",
    "    label_out = tf.concat([label_out_5],axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     winex = tf.concat([winex_5, winex_1, winex_2, winex_3, winex_4], axis=0)\n",
    "#     label_out = tf.concat([label_out_5,label_out_1, label_out_2, label_out_3, label_out_4],axis=0)\n",
    "\n",
    "    #classification head\n",
    "    d1 = tf_util.conv2d(winex, 512, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='dense1', bn_decay=bn_decay)\n",
    "    d2 = tf_util.conv2d(d1, 256, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='dense2', bn_decay=bn_decay)\n",
    "    d2=tf.layers.dropout(d2,rate=0.5, training=is_training, name='drop1')\n",
    "    \n",
    "    logits = tf_util.conv2d(d2, 1, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='out', bn_decay=bn_decay, activation_fn=None)\n",
    "    \n",
    "    logits = tf.squeeze(logits)\n",
    "    logits=tf.reshape(tf.squeeze(logits),(1,-1))\n",
    "    sig = tf.sigmoid(logits)\n",
    "\n",
    "    #sig = tf.reshape(tf.squeeze(sig),(-1,1))\n",
    "    y=tf.reshape(sig,[-1])\n",
    "    \n",
    "    max_ind = tf.argmax(y)\n",
    "    \n",
    "    tp = tf.equal(label_out[max_ind], 1)\n",
    "    \n",
    "    \n",
    "    differentiable_round = tf.maximum(y-0.499,0)\n",
    "    differentiable_round = differentiable_round * 10000\n",
    "    y_pred = tf.minimum(differentiable_round, 1)\n",
    "    #y_pred=tf.round(y)\n",
    "    \n",
    "    true_pos  = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(label_out,tf.bool),tf.cast(y_pred,tf.bool)),tf.float32))\n",
    "    \n",
    "    false_pos = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(tf.cast(label_out,tf.bool)),tf.cast(y_pred,tf.bool)),tf.float32))\n",
    "    true_neg  = tf.reduce_sum(tf.cast(tf.logical_not(tf.logical_or(tf.cast(label_out, tf.bool),\\\n",
    "                                                                   tf.cast(y_pred, tf.bool))),tf.float32))\n",
    "    false_neg = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not\\\n",
    "                                                     (tf.cast(y_pred,tf.bool)),tf.cast(label_out,tf.bool)),tf.float32))\n",
    "    return sig,logits, label_out,true_pos,false_pos,true_neg,false_neg, tp, max_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs=tf.placeholder(tf.float32,[None,2048,4])\n",
    "# idx_5 =tf.placeholder(tf.int32,[None,num_points_5,1])\n",
    "# is_training = tf.placeholder(tf.bool)\n",
    "# label_out_5 = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    num_points_5 =512\n",
    "#     num_points_1 = 512\n",
    "#     num_points_2 = 512\n",
    "#     num_points_3 = 512\n",
    "#     num_points_4 = 512\n",
    "    \n",
    "    inputs=tf.placeholder(tf.float32,[None,2048,4])#put placeholder\n",
    "\n",
    "    idx_5 =tf.placeholder(tf.int32,[None,num_points_5,1])\n",
    "\n",
    "#     idx_1 = tf.placeholder(tf.int32,[None,num_points_1,1])\n",
    "    \n",
    "#     idx_2 = tf.placeholder(tf.int32,[None,num_points_2,1])\n",
    "\n",
    "#     idx_3 = tf.placeholder(tf.int32,[None,num_points_3,1])\n",
    "    \n",
    "#     idx_4 = tf.placeholder(tf.int32,[None,num_points_4,1])\n",
    "\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "\n",
    "    label_out_5 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_1 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_2 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_3 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_4 = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    sig,logits, label_out,tp,fp,tn,fn, tp_num, maxi = classification_net(inputs, label_out_5, \\\n",
    "                                         idx_5, is_training)\n",
    "    \n",
    "    \n",
    "#     sig,logits, label_out,tp,fp,tn,fn = classification_net(inputs, label_out_5, label_out_1,label_out_2, label_out_3, label_out_4, \\\n",
    "#                                          idx_5, idx_1, idx_2, idx_3, idx_4, is_training)\n",
    "    \n",
    "    func_fl = focal_loss(tf.squeeze(sig),tf.squeeze(tf.cast(label_out, tf.float32)), gamma=0.75, alpha=0.25)\n",
    "    print(label_out.get_shape())\n",
    "#     loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     labels=tf.reshape(tf.cast(label_out,tf.float32),(-1,1)),\n",
    "#     logits=tf.reshape(logits,(-1,1))))\n",
    "\n",
    "    loss = tf.reduce_mean(func_fl)\n",
    "#     tpr=tf.divide(tf.cast(tp,tf.float32),tf.add(tp,fp))\n",
    "#     tnr=tf.divide(tf.cast(tn,tf.float32),tf.add(tn,fn))\n",
    "#     loss=tf.reduce_sum(tf.constant(1.0)-tf.scalar_mul(0.5,tf.add(tpr,tnr)))\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-5)\n",
    "    #train_step=optimizer.minimize(loss)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import debug\n",
    "from pyntcloud import PyntCloud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "# trix=[cc for cc in range(TRAIN_DATASET.__len__())]\n",
    "# shuffle(trix)\n",
    "# np.save('./Windows/trix', trix, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trix = np.load('./Windows/trix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trix=list(trix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon, 11 June 2018 02:17:34\n",
      "processing example number:  1\n",
      "processing example number:  2\n",
      "processing example number:  3\n",
      "processing example number:  4\n",
      "processing example number:  5\n",
      "processing example number:  6\n",
      "processing example number:  7\n",
      "processing example number:  8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cc0647f91a62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mpoint_set\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpoint_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0ms5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0me5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpoint_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorners_gt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-827b5c8b8def>\u001b[0m in \u001b[0;36mindices\u001b[0;34m(window_len, stride_len, point_set, labels, corners_gt)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindow_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0midtuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mmax_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mmin_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m     return _methods._amax(a, axis=axis,\n\u001b[0;32m-> 2320\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   2321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_amax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# small reductions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mumr_maximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_amin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "from tensorflow.python import debug as tf_debug\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "\n",
    "IOU_list = []\n",
    "point_cld = []\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "prob = []\n",
    "voxels_train=[]\n",
    "center_label=[]\n",
    "centroids=[]\n",
    "centroids_unique=[]\n",
    "corners=[]\n",
    "lbh=[]\n",
    "heading=[]\n",
    "extras=[]\n",
    "pedc=0\n",
    "selwin=[]  \n",
    "windows=[]\n",
    "residuallbh=[]\n",
    "for i in trix[:3*len(trix)//4]:\n",
    "    #while pedc<6000:\n",
    "        if TRAIN_DATASET.type_list[i]=='Pedestrian':\n",
    "            pedc+=1\n",
    "            print('processing example number: ',pedc)\n",
    "            point_set,labels,a,b,c,d,e,f,cls = TRAIN_DATASET.__getitem__(i)\n",
    "            point_set,labels,a,b,c,d,e,f,cls = TRAIN_DATASET.__getitem__(i)\n",
    "            corners_gt = TRAIN_DATASET.get_center_view_box3d(i)\n",
    "            #extras.append(TRAIN_DATASET.__getitem__(i))\n",
    "            index = np.argsort(point_set[:,2])\n",
    "            point_set= point_set[index]\n",
    "            labels = labels[index]\n",
    "            s5,e5,l = indices(1.0,0.1,point_set,labels, corners_gt)\n",
    "\n",
    "            if s5.size:\n",
    "\n",
    "                ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "                point_set = np.expand_dims(point_set, axis=0)\n",
    "                wc=0\n",
    "                for ci in range(len(l)):\n",
    "\n",
    "                    if l[ci]:\n",
    "                        while(wc<2):\n",
    "                            wc+=1\n",
    "                            #point_cld.append(point_set[0,ind_5[ci].reshape((-1)),:3])\n",
    "\n",
    "                            seg = point_set[0,ind_5[ci].reshape((-1))]\n",
    "                            #print(seg.shape)\n",
    "                            point_cld.append(seg)\n",
    "                            selwin.append(seg)\n",
    "                            \n",
    "#                             seg = pd.DataFrame(seg[:,:3])\n",
    "#                             seg.columns = ['x','y','z']\n",
    "#                             cloud = PyntCloud(seg)\n",
    "\n",
    "#                             voxelgrid_id = cloud.add_structure(\"voxelgrid\", size_x=0.05, size_y=0.05, size_z=0.05, regular_bounding_box=False)\n",
    "#                             voxelgrid = cloud.structures[voxelgrid_id]\n",
    "#                             voxels_train.append(voxelgrid)\n",
    "                            \n",
    "                            \n",
    "                            #print(pedc, seg.drop_duplicates().shape[0])\n",
    "                            #center_label.append(TRAIN_DATASET.get_center_view_box3d_center(i))\n",
    "                            #centroids_unique.append(np.mean(np.unique(np.array(point_set[0,ind_list[:,0]]),axis=0), axis=0))\n",
    "                            #centroids.append(np.mean(np.array(point_set[0,ind_list[:,0]]),axis=0))\n",
    "                            #corners.append(corners_gt)\n",
    "                            lbh.append(provider.class2size(d,e))\n",
    "                            extras.append(TRAIN_DATASET.__getitem__(i))\n",
    "                            #lbh.append(TRAIN_DATASET.__getitem__(i)[6])\n",
    "                            #cloud.plot()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(extras).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Windows/selwin',np.array(selwin), allow_pickle=True)\n",
    "\n",
    "np.save('./Windows/selextras',np.array(extras) , allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# lbh=np.load('./Windows/lbh_jtrue2.npy', allow_pickle=True)\n",
    "# voxels_train=np.load('./Windows/voxels_jtrue2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from random import shuffle\n",
    "import pandas as pd\n",
    "#logging.basicConfig(filename='run_1.log',level=logging.DEBUG)\n",
    "from tensorflow.python import debug as tf_debug\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "\n",
    "IOU_list = []\n",
    "point_cld = []\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "prob = []\n",
    "voxels_train=[]\n",
    "center_label=[]\n",
    "centroids=[]\n",
    "centroids_unique=[]\n",
    "corners=[]\n",
    "lbh=[]\n",
    "heading=[]\n",
    "extras=[]\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    #sess=tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "    # uncomment this when we want to load the checkpoint model\n",
    "    saver.restore(sess, \"./checkpoints/model_new_labels_pointwiseIOU_5_10000.ckpt\")\n",
    "    for j in range(6,7):   \n",
    "        total_tp=0\n",
    "        total_fp=0\n",
    "        total_tn=0\n",
    "        total_fn=0\n",
    "        total_tpnums = 0\n",
    "        total_fpnums = 0\n",
    "        total_fnnums = 0\n",
    "        \n",
    "        \n",
    "        #print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "        \n",
    "        #print('Epoch Number........................................',j)\n",
    "        pedc=0\n",
    "        \n",
    "        for i in trix:\n",
    "        #todo reformat code to have lists instead of many variable\n",
    "            if TRAIN_DATASET.type_list[i]=='Pedestrian':\n",
    "                pedc+=1\n",
    "                if pedc%500==0:\n",
    "                    total_tp=0\n",
    "                    total_fp=0\n",
    "                    total_tn=0\n",
    "                    total_fn=0\n",
    "                    total_tpnums = 0\n",
    "                    total_fpnums = 0\n",
    "                    total_fnnums = 0\n",
    "                point_set,labels,a,b,c,d,e,f,cls = TRAIN_DATASET.__getitem__(i)\n",
    "                #print(point_set.shape)\n",
    "                lbh.append(provider.class2size(d,e))\n",
    "                heading.append(provider.class2angle(b,c,12))\n",
    "                extras.append(TRAIN_DATASET.__getitem__(i))\n",
    "                index = np.argsort(point_set[:,2])\n",
    "                #raw_input()\n",
    "\n",
    "                point_set= point_set[index]\n",
    "                #print(point_set[:25])\n",
    "                labels = labels[index]\n",
    "\n",
    "                corners_gt = TRAIN_DATASET.get_center_view_box3d(i)\n",
    "                \n",
    "                #print(point_set.shape)\n",
    "\n",
    "                s5,e5,l = indices(1.0,0.1,point_set,labels, corners_gt)\n",
    "                \n",
    "                for u in range(len(s5)):\n",
    "                        aa = np.max(np.array(point_set[np.arange(s5[u], e5[u])]), axis=0)-np.min(np.array(point_set[np.arange(s5[u], e5[u])]), axis=0)\n",
    "                        #print(aa)\n",
    "                        #print('hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh')\n",
    "                        #print(aa)\n",
    "                        #if aa[2]>1.05:\n",
    "                        #    print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n",
    "                        #    print(aa)\n",
    "                \n",
    "                \n",
    "                #l1 = (l1>=0.6)\n",
    "                \n",
    "                #print('Label shape', l.shape)\n",
    "\n",
    "#                 s1, e1, l1 = indices(i, 1.1, 0.2)\n",
    "\n",
    "#                 s2, e2, l2 = indices(i, 1.3, 0.2)\n",
    "\n",
    "#                 s3, e3, l3 = indices(i, 0.7, 0.1)\n",
    "\n",
    "#                 s4, e4, l4 = indices(i, 0.5, 0.1)\n",
    "                \n",
    "                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "                    #print(ind_5.shape)\n",
    "                    for u in range(len(ind_5)):\n",
    "                        aa = np.max(np.array(point_set[ind_5[u]]), axis=0)-np.min(np.array(point_set[ind_5[u]]), axis=0)\n",
    "                        #print(aa)\n",
    "                        if aa[0,2]>1.05:\n",
    "                            print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "                            print(aa)\n",
    "                            \n",
    "                            \n",
    "\n",
    "#                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "#                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "\n",
    "                    point_set = np.expand_dims(point_set, axis=0)\n",
    "                        \n",
    "                    x,tpos,fpos,tneg,fneg, tpnums, maxind =sess.run([ loss,tp,fp,tn,fn, tp_num, maxi],feed_dict={idx_5:ind_5,\\\n",
    "                                                               label_out_5: l.astype(np.int32),\\\n",
    "                                                               inputs:point_set, is_training: True})\n",
    "                    \n",
    "                    #print(point_set.shape)\n",
    "                    ind_list = ind_5[maxind]\n",
    "                    #print(np.max(ind_list)-np.min(ind_list))\n",
    "                    #print(ind_list[0])\n",
    "                    #print(ind_list.shape)\n",
    "                    seg = point_set[0,ind_list[:,0]]\n",
    "                    #print(seg.shape)\n",
    "                    point_cld.append(seg)\n",
    "                    \n",
    "                    seg = pd.DataFrame(seg[:,:3])\n",
    "                    seg.columns = ['x','y','z']\n",
    "                    cloud = PyntCloud(seg)\n",
    "                    cloud.plot()\n",
    "                    voxelgrid_id = cloud.add_structure(\"voxelgrid\", size_x=0.05, size_y=0.05, size_z=0.05, regular_bounding_box=False)\n",
    "                    voxelgrid = cloud.structures[voxelgrid_id]\n",
    "                    voxels_train.append(voxelgrid)\n",
    "                    print(pedc, seg.drop_duplicates().shape[0])\n",
    "                    center_label.append(TRAIN_DATASET.get_center_view_box3d_center(i))\n",
    "                    centroids_unique.append(np.mean(np.unique(np.array(point_set[0,ind_list[:,0]]),axis=0), axis=0))\n",
    "                    centroids.append(np.mean(np.array(point_set[0,ind_list[:,0]]),axis=0))\n",
    "                    corners.append(corners_gt)\n",
    "                    \n",
    "                    #heading angle\n",
    "                    \n",
    "                    #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voxels_train_np=[i.get_feature_vector() for i in voxels_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_voxels=[]\n",
    "#obj_centroids=[]\n",
    "#obj_centroids_unique=[]\n",
    "#obj_center_label=[]\n",
    "#obj_corners=[]\n",
    "obj_lbh=[]\n",
    "obj_extras=[]\n",
    "#obj_extras=[]\n",
    "#obj_heading=[]\n",
    "for i in range(len(voxels_train)):\n",
    "    try :\n",
    "        obj_voxels.append(voxels_train[i].get_feature_vector(mode='density'))\n",
    "        #obj_centroids.append(centroids[i])\n",
    "        #obj_centroids_unique.append(centroids_unique[i])\n",
    "        #obj_center_label.append(center_label[i])\n",
    "        #obj_corners.append(corners[i])\n",
    "        obj_lbh.append(lbh[i])\n",
    "        obj_extras.append(extras[i])\n",
    "        #obj_extras.append(extras[i])\n",
    "        #obj_heading.append(heading[i])\n",
    "    except IndexError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Windows/voxels_denstrue2',np.array(obj_voxels), allow_pickle=True)\n",
    "\n",
    "np.save('./Windows/lbh_denstrue2',np.array(obj_lbh) , allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lbh=np.load('./Windows/lbh_jtrue2.npy', allow_pickle=True)\n",
    "voxels_train=np.load('./Windows/voxels_jtrue2.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del voxels_train\n",
    "del lbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_voxels),len(obj_centroids),len(obj_centroids_unique),len(obj_center_label),len(obj_corners),len(obj_lbh),len(obj_extras),len(obj_heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids)[:,:3])**2, axis=1)))\n",
    "print(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids)[:,:3])**2, axis=1)))\n",
    "print(np.mean(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids)[:,:3])**2, axis=1))))\n",
    "plt.hist(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids)[:,:3])**2, axis=1)), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check the distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids_unique)[:,:3])**2, axis=1)))\n",
    "print(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids_unique)[:,:3])**2, axis=1)))\n",
    "print(np.mean(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids_unique)[:,:3])**2, axis=1))))\n",
    "plt.hist(np.sqrt(np.sum((np.array(obj_center_label)-np.array(obj_centroids_unique)[:,:3])**2, axis=1)), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(voxels_train)):\n",
    "#     if i not in [289,315,431,728,746,1283,1543,1865,2128,3485,3753,4429,4928,5048,5432,5907,6191,6999,7100,7574,\\\n",
    "#                 7687,9404,10179]:\n",
    "#         voxels_train[i].get_feature_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_voxels = []\n",
    "# center_label = []\n",
    "# centroids= []\n",
    "# print(np.array(point_cld[0]).shape)\n",
    "# for i in range(len(voxels_train)):\n",
    "#     if i not in [289,315,431,728,746,1283,1543,1865,2128,3485,3753,4429,4928,5048,5432,5907,6191,6999,7100,7574,\\\n",
    "#                 7687,9404,10179]:\n",
    "#         obj_voxels.append(voxels_train[i].get_feature_vector())\n",
    "#         #center_label.append(TRAIN_DATASET.get_center_view_box3d_center(trix[i]))\n",
    "#         #centroids.append(np.mean(np.unique(np.array(point_cld[i]),axis=0), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_voxels_modified = []\n",
    "#center_label_modified = []\n",
    "#centroids_modified = []\n",
    "#obj_corners_modified=[]\n",
    "obj_lbh_modified=[]\n",
    "#obj_extras_modified=[]\n",
    "#obj_heading_modified=[]\n",
    "\n",
    "for i in range(len(obj_voxels)):\n",
    "    dist = np.sqrt(np.sum((np.array(obj_center_label[i])-np.array(obj_centroids)[i:i+1,:3])**2, axis=1))\n",
    "    if dist<1:\n",
    "        #print(dist)\n",
    "        obj_voxels_modified.append(obj_voxels[i])\n",
    "        center_label_modified.append(obj_center_label[i])\n",
    "        centroids_modified.append(obj_centroids[i])\n",
    "        obj_corners_modified.append(obj_corners[i])\n",
    "        obj_lbh_modified.append(obj_lbh[i])\n",
    "        obj_extras_modified.append(obj_extras[i])\n",
    "        obj_heading_modified.append(obj_heading[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(obj_voxels_modified),len(center_label_modified),len(centroids_modified),len(obj_corners_modified),len(obj_lbh_modified),len(obj_extras_modified),len(obj_heading_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_x,max_y,max_z=0,0,0\n",
    "for v in obj_voxels:\n",
    "    max_x=max(max_x,v.shape[0])\n",
    "    max_y=max(max_y,v.shape[1])\n",
    "    max_z=max(max_z,v.shape[2])\n",
    "print( max_x,max_y,max_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pad_shape=[30,45,20]\n",
    "padded_voxels=[]\n",
    "ccc=0\n",
    "for v in obj_voxels:\n",
    "    ccc+=1\n",
    "    print(ccc)\n",
    "    dx=pad_shape[0]-v.shape[0]\n",
    "    dy=pad_shape[1]-v.shape[1]\n",
    "    dz=pad_shape[2]-v.shape[2]\n",
    "    npad=((dx//2,dx-dx//2),(dy//2,dy-dy//2),(dz//2,dz-dz//2))\n",
    "    padded_voxels.append(np.pad(v,pad_width=npad,mode='constant',constant_values=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Windows/padded_voxels_densetrue',np.array(padded_voxels), allow_pickle=True)\n",
    "np.save('./Windows/fulllbh_densetrue',np.array(obj_lbh) , allow_pickle=True)\n",
    "np.save('./Windows/extras_densetrue',np.array(obj_extras) , allow_pickle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=np.load('./Windows/lbh_densetrue.npy', allow_pickle=True)\n",
    "v=np.load('./Windows/padded_voxels_densetrue.npy', allow_pickle=True)\n",
    "v=np.load('./Windows/extras_densetrue.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_corners_modified[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_corners_modified[0][:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.unique(obj_corners_modified[0][:,1],1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testi=[]\n",
    "for i in range(100):\n",
    "    if TRAIN_DATASET.__getitem__(trix[i])[-1][1]:\n",
    "        testi.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET.__getitem__(trix[4])[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in testi:\n",
    "    print(provider.class2size(TRAIN_DATASET.__getitem__(trix[i])[5],TRAIN_DATASET.__getitem__(trix[i])[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET.get_center_view_box3d(trix[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_corners_modified[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in obj_corners_modified[:len(testi)]:\n",
    "    print(finddim(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finddim(corners):\n",
    "    c=np.copy(corners)\n",
    "    height=c[:,1].max()-c[:,1].min()\n",
    "    fourp=np.unique(np.delete(c,1,1),axis=0)\n",
    "    point1=fourp[0]\n",
    "    d=[]\n",
    "    for point2 in fourp[1:]:\n",
    "        d.append(np.linalg.norm(point1-point2))\n",
    "    d=sorted(d)\n",
    "    breadth=d[0]\n",
    "    length=d[1]\n",
    "    return np.array([length,breadth,height])\n",
    "lbh=[]\n",
    "for c in obj_corners_modified:\n",
    "    lbh.append(finddim(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_centers=[]\n",
    "\n",
    "for i in range(len(centroids_modified)):\n",
    "    residual_centers.append(center_label_modified[i]-np.array(centroids_modified)[i,:3])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for residue in residual_centers:\n",
    "    if np.sqrt(np.sum(residue**2))>1:\n",
    "        print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Windows/centers',center_label_modified, allow_pickle=True)\n",
    "np.save('./Windows/centroids',centroids_modified , allow_pickle=True)\n",
    "np.save('./Windows/corners',obj_corners_modified, allow_pickle=True)\n",
    "np.save('./Windows/lbh',obj_lbh_modified, allow_pickle=True)\n",
    "np.save('./Windows/extras',obj_extras_modified, allow_pickle=True)\n",
    "np.save('./Windows/heading_angle',obj_heading_modified, allow_pickle=True)\n",
    "np.save('./Windows/res_centerlabels', residual_centers, allow_pickle=True)\n",
    "np.save('./Windows/voxels', padded_voxels, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rdist,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./Windows/voxels', padded_voxels, allow_pickle=True)\n",
    "# np.save('./Windows/centerlabels', residual_centers, allow_pickle=True)\n",
    "# np.save('./Windows/lbh', lbh, allow_pickle=True)\n",
    "# np.save('./Windows/corners', obj_corners_modified, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in padded_voxels:\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linalg.norm(np.array(center_label[:1]), np.array(centroids)[:1,:3], axis=1)\n",
    "print(center_label[:1])\n",
    "print(np.array(centroids)[:1,:3])\n",
    "print(np.sum((np.array(center_label[:1])-np.array(centroids)[:1,:3])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c,d,e,f,g,h,i = TRAIN_DATASET.__getitem__(100)\n",
    "print(c)\n",
    "print(TRAIN_DATASET.get_center_view_box3d_center(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.unique(point_cld[i],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnge= []\n",
    "for i in range(len(point_cld)):\n",
    "    rnge.append(np.max(np.array(point_cld[i]), axis=0)-np.min(np.array(point_cld[i]), axis=0))\n",
    "print(len(rnge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(rnge), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(np.array(point_cld[0])[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.array(point_cld[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = []\n",
    "for i in range(len(point_cld)):\n",
    "    o = np.max(np.array(point_cld[i])[:,2], axis=0)-np.min(np.array(point_cld[i])[:,2], axis=0)\n",
    "    if o>1.0:\n",
    "        outliers.append([i,o])\n",
    "print(len(outliers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 10\n",
    "print(voxels_train[ex].get_feature_vector().shape)\n",
    "plotFromVoxels(voxels_train[ex].get_feature_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j= 0\n",
    "for i in range(1000):\n",
    "    #print(TRAIN_DATASET.type_list[trix[i]])\n",
    "    if TRAIN_DATASET.type_list[trix[i]]=='Pedestrian':\n",
    "        if j==60:\n",
    "            print(j)\n",
    "            print(i)\n",
    "            seg,a,b,c,d,e,f,g,h = TRAIN_DATASET[60]     \n",
    "            seg = pd.DataFrame(seg[:,:3])\n",
    "            seg.columns = ['x','y','z']\n",
    "            cloud = PyntCloud(seg)\n",
    "            cloud.plot()\n",
    "                \n",
    "            \n",
    "        j += 1\n",
    "ex  = 60\n",
    "seg,a,b,c,d,e,f,g,h = TRAIN_DATASET[trix[386]]\n",
    "print(seg.shape)                    \n",
    "seg = pd.DataFrame(seg[:,:3])\n",
    "seg.columns = ['x','y','z']\n",
    "cloud = PyntCloud(seg)\n",
    "cloud.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import scipy.ndimage as nd\n",
    "import scipy.io as io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.measure as sk\n",
    "\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import trimesh\n",
    "from stl import mesh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFromVF(vertices, faces):\n",
    "    input_vec = mesh.Mesh(np.zeros(faces.shape[0], dtype=mesh.Mesh.dtype))\n",
    "    for i, f in enumerate(faces):\n",
    "        for j in range(3):\n",
    "            input_vec.vectors[i][j] = vertices[f[j],:]\n",
    "    figure = plt.figure()\n",
    "    axes = mplot3d.Axes3D(figure)\n",
    "    axes.add_collection3d(mplot3d.art3d.Poly3DCollection(input_vec.vectors))\n",
    "    scale = input_vec.points.flatten(-1)\n",
    "    axes.auto_scale_xyz(scale, scale, scale)\n",
    "    plt.show()\n",
    "\n",
    "def plotFromVoxels(voxels):\n",
    "    #print(voxels.nonzero())\n",
    "    x,y,z = voxels.nonzero()\n",
    "    fig = plt.figure()\n",
    "    #ax = fig.add_subplot(111, projection='3d')\n",
    "    #ax.scatter( 0.05*x, z*0.05, -0.1*y, zdir='z', c= 'red', norm=0.05)\n",
    "    ax = fig.gca(projection='3d')\n",
    "    ax.voxels(0.05*voxels)\n",
    "    plt.show()\n",
    "\n",
    "def getVFByMarchingCubes(voxels, threshold=0.5):\n",
    "    #print(len(sk.marching_cubes(voxels, level=threshold)))\n",
    "    v, f, n, m =  sk.marching_cubes(voxels, level=threshold)\n",
    "    return v, f\n",
    "\n",
    "def plotMeshFromVoxels(voxels, threshold=0.5):\n",
    "    #print(getVFByMarchingCubes(voxels, threshold))\n",
    "    v,f = getVFByMarchingCubes(voxels, threshold)\n",
    "    plotFromVF(v,f)\n",
    "\n",
    "def plotVoxelVisdom(voxels, visdom, title):\n",
    "    v, f = getVFByMarchingCubes(voxels)\n",
    "    visdom.mesh(X=v, Y=f, opts=dict(opacity=0.5, title=title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotFromVoxels(np.array([[[0,1,0],[0,0,0],[0,1,1]],[[1,1,1],[1,1,1],[0,1,1]]]))\n",
    "plotMeshFromVoxels(np.array([[[0,1,0],[0,0,0],[0,1,1]],[[1,1,1],[1,1,1],[0,1,1]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelgrid_id = cloud.add_structure(\"voxelgrid\", size_x=0.05, size_y=0.05, size_z=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg['z'].max()-seg['z'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelgrid = cloud.structures[voxelgrid_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelgrid.get_feature_vector().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seg.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc_voxels():\n",
    "    for voxel in voxels_train:\n",
    "        dim=voxel.shape\n",
    "        temp=np.zeros((30,40,20))\n",
    "        temp\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(voxelgrid.get_feature_vector().shape)\n",
    "print(np.sum(voxelgrid.get_feature_vector()))\n",
    "print(voxelgrid.get_feature_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelgrid.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from random import shuffle\n",
    "#logging.basicConfig(filename='run_1.log',level=logging.DEBUG)\n",
    "from tensorflow.python import debug as tf_debug\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "\n",
    "IOU_list = []\n",
    "saver = tf.train.Saver()\n",
    "trix=[cc for cc in range(TRAIN_DATASET.__len__())]\n",
    "shuffle(trix)\n",
    "prob = []\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    #sess=tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print(\"Variable: \", k)\n",
    "        print(\"Shape: \", v.shape)\n",
    "    # uncomment this when we want to load the checkpoint model\n",
    "    saver.restore(sess, \"./checkpoints/model_new_labels_pointwiseIOU_5_10000.ckpt\")\n",
    "\n",
    "    \n",
    "    for j in range(6,15):   \n",
    "        total_tp=0\n",
    "        total_fp=0\n",
    "        total_tn=0\n",
    "        total_fn=0\n",
    "        total_tpnums = 0\n",
    "        total_fpnums = 0\n",
    "        total_fnnums = 0\n",
    "        \n",
    "        \n",
    "        print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "        \n",
    "        print('Epoch Number........................................',j)\n",
    "        pedc=0\n",
    "        \n",
    "        for i in trix:\n",
    "        #todo reformat code to have lists instead of many variable\n",
    "            if TRAIN_DATASET.type_list[i]=='Pedestrian':\n",
    "                pedc+=1\n",
    "                if pedc%500==0:\n",
    "                    total_tp=0\n",
    "                    total_fp=0\n",
    "                    total_tn=0\n",
    "                    total_fn=0\n",
    "                    total_tpnums = 0\n",
    "                    total_fpnums = 0\n",
    "                    total_fnnums = 0\n",
    "                point_set,labels,a,b,c,d,e,f,cls = TRAIN_DATASET.__getitem__(i)\n",
    "\n",
    "                index = np.argsort(point_set[:,2])\n",
    "                point_set = point_set[index]\n",
    "                labels = labels[index]\n",
    "\n",
    "                corners_gt = TRAIN_DATASET.get_center_view_box3d(i)\n",
    "                \n",
    " \n",
    "                s5,e5,l = indices(1.0,0.1,point_set,labels, corners_gt)\n",
    "        \n",
    "                #l1 = (l1>=0.6)\n",
    "                \n",
    "                print('Label shape', l.shape)\n",
    "\n",
    "\n",
    "                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "\n",
    "\n",
    "                    point_set = np.expand_dims(point_set, axis=0)\n",
    "                        \n",
    "                    _, x,tpos,fpos,tneg,fneg, tpnums, maxind =sess.run([train_op, loss,tp,fp,tn,fn, tp_num, maxi],feed_dict={idx_5:ind_5,\\\n",
    "                                                               label_out_5: l.astype(np.int32),\\\n",
    "                                                               inputs:point_set, is_training: True})\n",
    "                    total_tp+=tpos\n",
    "                    total_fp+=fpos\n",
    "                    total_tn+=tneg\n",
    "                    total_fn+=fneg\n",
    "                    total_tpnums+= tpnums\n",
    "                    total_fpnums+= (1-tpnums)\n",
    "                    total_fnnums+= (1-tpnums)\n",
    "                    ind_5[maxind]\n",
    "                    print('----------------------------------------------------------------------------------')\n",
    "                    print('Epoch ', j , 'Loss for the current example ',i,' = ', x,'pedc = ',pedc)\n",
    "                    #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                    print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                    #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                    #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                    print('Cumulative metrics: Precision = ',(total_tp*1.0)/(total_tp+total_fp),\\\n",
    "                          ' Recall = ',(total_tp*1.0)/(total_tp+total_fn))\n",
    "                    #logging.info('Cumulative metrics: Precision = ',(total_tp*1.0)/(total_tp+total_fp),\\\n",
    "                    #      ' Recall = ',(total_tp*1.0)/(total_tp+total_fn))\n",
    "                    print('Single metrics: Precision = ',(total_tpnums*1.0)/(total_tpnums+total_fpnums),\\\n",
    "                          ' Recall = ',(total_tpnums*1.0)/(total_tpnums+total_fnnums))\n",
    "                    print('----------------------------------------------------------------------------------')\n",
    "                    \n",
    "                    if(pedc%5000==0):\n",
    "                        save_path = saver.save(sess, \"./checkpoints/model_new_labels_pointwiseIOU_\"+str(j)+'_'+str(pedc)+\".ckpt\")\n",
    "                    \n",
    "\n",
    "                #print(c)\n",
    "                #print(n)\n",
    "            #i += 1\n",
    "        #-----------------------------------------------------------------------------------------------#\n",
    "        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "        k = 0\n",
    "        Vtotal_tp=0\n",
    "        Vtotal_fp=0\n",
    "        Vtotal_tn=0\n",
    "        Vtotal_fn=0\n",
    "        Vtotal_tpnums = 0\n",
    "        Vtotal_fpnums = 0\n",
    "        Vtotal_fnnums = 0\n",
    "        while k<len(VAL_DATASET):\n",
    "        #todo reformat code to have lists instead of many variable\n",
    "\n",
    "            if VAL_DATASET.type_list[k]=='Pedestrian':\n",
    "\n",
    "                point_set,labels,a,b,c,d,e,f,cls = VAL_DATASET.__getitem__(k)\n",
    "\n",
    "                index = np.argsort(point_set[:,2])\n",
    "                point_set = point_set[index]\n",
    "                labels = labels[index]\n",
    "\n",
    "                \n",
    "                corners_gt = VAL_DATASET.get_center_view_box3d(k)\n",
    "                \n",
    "                #print(point_set.shape)\n",
    "\n",
    "                s5,e5,l = indices(1.0,0.1,point_set,labels, corners_gt)\n",
    "                \n",
    "                #l1 = l1>0.5\n",
    "\n",
    "#                 s1, e1, l1 = indices(k, 1.1, 0.2, 'val')\n",
    "\n",
    "#                 s2, e2, l2 = indices(k, 1.3, 0.2, 'val')\n",
    "\n",
    "#                 s3, e3, l3 = indices(k, 0.7, 0.1, 'val')\n",
    "\n",
    "#                 s4, e4, l4 = indices(k, 0.5, 0.1, 'val')\n",
    "\n",
    "                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "#                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "\n",
    "#                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "\n",
    "                    point_set = np.expand_dims(point_set, axis=0)\n",
    "                    print(Vtotal_tp)\n",
    "                    x,tpos,fpos,tneg,fneg, tp_nums, maxind =sess.run([loss,tp,fp,tn,fn, tp_num, maxi],feed_dict={idx_5:ind_5, \\\n",
    "                                                               label_out_5: l.astype(np.int32), inputs:point_set, is_training: True})\n",
    "\n",
    "\n",
    "                    #window_points = ind_5[maxind]\n",
    "\n",
    "                    #window_points = point_set[0,ind_5[maxind].reshape((-1))][:,:3]\n",
    "                    #min_x, min_y, min_z = np.min(window_points,axis=0)\n",
    "                    #max_x, max_y, max_z = np.max(window_points,axis=0)\n",
    "                    #corners = [[min_x,max_y,max_z],[min_x,max_y,min_z],[max_x,max_y,min_z],[max_x,max_y,max_z],\\\n",
    "                    #           [min_x,min_y,max_z],[min_x,min_y,min_z],[max_x,min_y,min_z],[max_x,min_y,max_z]]\n",
    "                    #corners=np.array(corners)\n",
    "                    #corners_gt = VAL_DATASET.get_center_view_box3d(k)\n",
    "                    #IOU, IOU_2D, inter, inter_vol = box_util.box3d_iou(corners, corners_gt)\n",
    "                    #print('IOUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU')\n",
    "                    #print(IOU, IOU_2D, inter, inter_vol)\n",
    "                    #IOU_list.append([IOU, IOU_2D, inter, inter_vol, k])\n",
    "                    #if inter>0.4:\n",
    "                    #    c += 1\n",
    "                    #    if inter>0.5:\n",
    "                    #        c1 += 1\n",
    "                    #if inter_vol>0.4:\n",
    "                    #    c2 += 1\n",
    "                    #    if inter_vol>0.5:\n",
    "                    #        c3 +=1 \n",
    "\n",
    "                    print(Vtotal_tp)\n",
    "                    Vtotal_tp +=tpos\n",
    "                    Vtotal_fp +=fpos\n",
    "                    Vtotal_tn +=tneg\n",
    "                    Vtotal_fn +=fneg\n",
    "                    Vtotal_tpnums+= tp_nums\n",
    "                    Vtotal_fpnums+= (1-tp_nums)\n",
    "                    Vtotal_fnnums+= (1-tp_nums)\n",
    "                    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "                    print('Epoch ', j , 'Loss for the current example ',k,' = ', x)\n",
    "                    #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                    print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                    #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                    #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                    print('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                          ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "\n",
    "                    print('Single metrics: Precision = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fpnums),\\\n",
    "                          ' Recall = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fnnums))\n",
    "                    #logging.info('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                    #      ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------#\n",
    "#         prob = sess.run([sig],feed_dict={idx_5:ind_5, idx_1:ind_1, idx_2:ind_2, idx_3:ind_3, idx_4:ind_4,\\\n",
    "#                                                        label_out_5: l5.astype(np.int32), label_out_1: l1.astype(np.int32),\\\n",
    "#                                                        label_out_2: l2.astype(np.int32), label_out_3: l3.astype(np.int32),\\\n",
    "#                                                        label_out_4: l4.astype(np.int32), inputs:point_set,  is_training: False})\n",
    "#         print(prob)\n",
    "        print(ind_5)\n",
    "        print(label_out_5)\n",
    "        prob = sess.run([sig],feed_dict={idx_5:ind_5,  \\\n",
    "                                                       label_out_5: l.astype(np.int32), inputs:point_set,  is_training: True})\n",
    "        print(prob)\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.random.rand(31,26,12)\n",
    "padded_n=np.zeros((40,30,20))\n",
    "padded_n[:min(n.shape[0],40),:min(n.shape[1],30),:min(n.shape[2],20)]=n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=tf.placeholder(tf.float32,shhape=(1,30,45,20))\n",
    "\n",
    "conv3d1=tf_util.conv3d(inp,num_output_channels=64,kernel_size=[3,3,3],scope='convr1', is_training = True)\n",
    "conv3d2=tf_util.conv3d(conv3d1,num_output_channels=128,kernel_size=[3,3,3],scope='convr2', is_training = True)\n",
    "conv3d3=tf_util.conv3d(conv3d2,num_output_channels=256,kernel_size=[3,3,3],scope='convr3', is_training = True)\n",
    "\n",
    "dense=conv2d(conv3d2,num_output_channels=30,kernel_size=[40,40,30],scope='dense1r', is_training = True)\n",
    "out=conv2d(dense,num_output_channels=2,kernel_size=[1,1,30],scope='dense2r', is_training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyntcloud\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "from train import provider\n",
    "from train import box_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from model_util import huber_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import tf_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers=np.load('./Windows/centers.npy', allow_pickle=True)\n",
    "centroids=np.load('./Windows/centroids.npy', allow_pickle=True)\n",
    "corners=np.load('./Windows/corners.npy', allow_pickle=True)\n",
    "lbh=np.load('./Windows/lbh.npy', allow_pickle=True)\n",
    "extras=np.load('./Windows/extras.npy', allow_pickle=True)\n",
    "heading_angle =np.load('./Windows/heading_angle.npy',allow_pickle=True)\n",
    "residual_centers=np.load('./Windows/res_centerlabels.npy',  allow_pickle=True)\n",
    "padded_voxels=np.load('./Windows/voxels.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_lbh=np.array([x[6] for x in extras])\n",
    "residual_angle=np.array([x[4] for x in extras])\n",
    "angle_class=np.array([x[3] for x in extras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_angle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_onehot_angle_bin(x):\n",
    "    temp=np.zeros((12))\n",
    "    temp[x]=1\n",
    "    return temp\n",
    "one_hot_angle_class=[conv_onehot_angle_bin(x) for x in angle_class]\n",
    "residual_angle_labels=[one_hot_angle_class[i]*residual_angle[i] for i in range(residual_angle.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_angle_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_angle_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_centers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.linalg.norm(residual_lbh,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def create_lbl(i):\n",
    "#    temp=np.zeros(30)\n",
    "#    temp[:]\n",
    "    \n",
    "targets = np.zeros((len(residual_angle_labels),30))\n",
    "targets[:,:3] = residual_centers\n",
    "targets[:,3:6] = residual_lbh\n",
    "targets[:,6:18] = one_hot_angle_class\n",
    "targets[:,18:30] = residual_angle_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBoxRegression(inp):\n",
    "    conv3d1=tf_util.conv3d(inp,num_output_channels=16,kernel_size=[2,2,2], scope='convr1', bn=True, is_training = True, bn_decay=True)\n",
    "    #conv3d1 = tf_util.max_pool3d(conv3d1, [2,2,2], scope='maxpool1', stride= [2,2,2])\n",
    "    conv3d11=tf_util.conv3d(inp,num_output_channels=16,kernel_size=[1,2,1], scope='convr11', bn=True, is_training = True, bn_decay=True)\n",
    "    conv3d1o=tf.concat([conv3d1,conv3d11 ],axis=4)\n",
    "    conv3d11 = tf_util.max_pool3d(conv3d1o, [2,2,2], scope='maxpool1', stride= [2,2,2])\n",
    "    \n",
    "    conv3d2=tf_util.conv3d(conv3d1,num_output_channels=128,kernel_size=[3,3,3], scope='convr2', bn=True, is_training = True, bn_decay=True)\n",
    "    #conv3d2 = tf_util.max_pool3d(conv3d2, [2,2,2], scope='maxpool2', stride= [2,2,2])\n",
    "    conv3d3=tf_util.conv3d(conv3d2,num_output_channels=256,kernel_size=[2,3,2], scope='convr3', bn=True, is_training = True, bn_decay=True)\n",
    "    \n",
    "    #deconv3d1=tf_util.conv2d_transpose(conv3d1,128,kernel_size=[1,1])\n",
    "    #deconv3d2=tf_util.conv2d_transpose(conv3d2,128,)\n",
    "    \n",
    "    \n",
    "    \n",
    "    dense1 = tf.contrib.layers.flatten(conv3d3)\n",
    "    dense2 = tf_util.fully_connected(dense1, 64, scope='denser2', bn=True, is_training=True, bn_decay=True)\n",
    "    #dense3 = tf_util.fully_connected(dense2,128 , scope='denser3', bn=True, is_training=True, bn_decay=True)\n",
    "    dense4 = tf_util.fully_connected(dense2, 30, activation_fn=None, scope='denser4')\n",
    "    \n",
    "    \n",
    "    return dense4,conv3d1o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_util import g_type_mean_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "\n",
    "    voxel_inputs =tf.placeholder(tf.float32,[None,30,45,20, 1])#put placeholder\n",
    "\n",
    "    regress_labels =tf.placeholder(tf.float32,[None,30])\n",
    "\n",
    "    out,abc = boundingBoxRegression(voxel_inputs)\n",
    "    abc=abc.get_shape()\n",
    "    #loss = tf.losses.mean_squared_error(regress_labels, out, scope='loss')\n",
    "    center_loss1=tf.norm(out-regress_labels,axis=1)#out[0:3]\n",
    "    center_loss=tf.reduce_mean(center_loss1)\n",
    "    center_hloss=huber_loss(center_loss1,delta=1.0)\n",
    "    \n",
    "    \n",
    "    residual_sizes_labels=regress_labels[:,3:6]\n",
    "    \n",
    "    #residual_size_labels_normalized=tf.div(residual_sizes_labels,tf.constant(g_type_mean_size['Pedestrian'],dtype=tf.float32))\n",
    "    size_loss1=tf.norm(out[:,3:6]-residual_sizes_labels, axis=1)\n",
    "\n",
    "    #size_loss1=tf.norm(out[:,3:6]-residual_size_labels_normalized, axis=1)\n",
    "    #size_hloss=huber_loss(size_loss1,delta=1.0)\n",
    "    size_loss=tf.reduce_mean(size_loss1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    heading_class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=regress_labels[:,6:18],logits=out[:,6:18]))\n",
    "    \n",
    "    heading_residual_label=regress_labels[:,18:30]\n",
    "    heading_residual_normalized_label=heading_residual_label/(np.pi/12)\n",
    "    heading_hloss = huber_loss(tf.reduce_sum(tf.norm(out[:,18:30]-heading_residual_normalized_label,axis=1)*tf.to_float(regress_labels[:,6:18]), axis=1), delta=1.0)\n",
    "    ####loss = tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.pow(out-regress_labels,2), axis=1)))\n",
    "    \n",
    "                                           \n",
    "    #corner loss\n",
    "                                         \n",
    "    # Volume loss\n",
    "    \n",
    "    loss=1*size_loss#+0*heading_hloss+0*heading_class_loss+0*center_hloss\n",
    "\n",
    "    #loss=0*center_hloss+1*size_hloss+0*heading_hloss+0*heading_class_loss\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "    #train_step=optimizer.minimize(loss)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    \n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_center = {}\n",
    "test_pred_center = {}\n",
    "all_losses_train = {}\n",
    "all_losses_test={}\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = sess.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "    # uncomment this when we want to load the checkpoint model\n",
    "    #saver.restore(sess, \"./checkpoints/regression_\"+str(0)+'_'+str(4100)+\".ckpt\")\n",
    "    vox = np.load('./Windows/voxels.npy')\n",
    "    #labels = np.load('./Windows/centerlabels.npy')\n",
    "    assert vox.shape[0]==targets.shape[0]\n",
    "    assert vox[0].shape==(30,45,20)\n",
    "    \n",
    "    for epoch in range(40):\n",
    "        print('**************************************************************************************')\n",
    "        i=0\n",
    "        batch_size=1\n",
    "        pred_center[epoch] = []\n",
    "        all_losses_train[epoch] = []\n",
    "        while(i<(vox.shape[0]-2000)):\n",
    "            _, l,s_loss,c_loss,a, b,hc_loss, pred = sess.run([train_op, loss, size_loss, center_loss,size_loss1,residual_sizes_labels, heading_class_loss, out], feed_dict={voxel_inputs:np.reshape(vox[i:i+batch_size], (batch_size, vox[i:i+batch_size].shape[1],\\\n",
    "                                                                                        vox[i:i+batch_size].shape[2], vox[i:i+batch_size].shape[3], 1)),\\\n",
    "                                                       regress_labels: np.reshape(targets[i:i+batch_size],(batch_size,-1))})\n",
    "#             _, l,s_loss,c_loss,a, b,hc_loss, pred = sess.run([train_op, loss, size_loss, center_loss,size_loss1,residual_sizes_labels, heading_class_loss, out], feed_dict={voxel_inputs:np.reshape(vox[i:i+batch_size], (batch_size, vox[i:i+batch_size].shape[1],\\\n",
    "#                                                                                         vox[i:i+batch_size].shape[2], vox[i:i+batch_size].shape[3], 1)),\\\n",
    "#                                                        regress_labels: np.reshape(targets[i:i+batch_size],(batch_size,-1))})\n",
    "            i+=batch_size\n",
    "            #print('epoch=',epoch,'batch=',i,'loss=',l)\n",
    "            #print('OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO')\n",
    "            #print(a)\n",
    "            #print(b)\n",
    "            #print(pred[:,3:6])\n",
    "            print(l)\n",
    "            #print('OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO')\n",
    "            pred_center[epoch].append([pred,i,l])\n",
    "            all_losses_train[epoch].append([s_loss,c_loss,hc_loss])\n",
    "            if i%100<5:\n",
    "                save_path = saver.save(sess, \"./checkpoints/regression_\"+str(epoch)+'_'+str(i)+\".ckpt\")\n",
    "                print('%%%%%%%%%%%%%%%%%%%%%%%%%%AverageLoss%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "                print(np.mean(np.array(all_losses_train[epoch]),axis=0))\n",
    "                np.save('./Windows/train_pred_'+str(epoch), np.array(pred_center[epoch]), allow_pickle=True)\n",
    "                np.save('./Windows/train_loss_'+str(epoch), np.array(all_losses_train[epoch]), allow_pickle=True)\n",
    "                \n",
    "                #print(np.mean(np.array(pred_center[epoch]), axis=0)[2])\n",
    "        test=vox.shape[0]-2000\n",
    "        test_pred_center[epoch]=[]\n",
    "        all_losses_test[epoch] = []\n",
    "\n",
    "        while(test<(vox.shape[0])):\n",
    "            l,s_loss,c_loss,hc_loss, pred = sess.run([loss, size_loss, center_loss, heading_class_loss,out], feed_dict={voxel_inputs:np.reshape(vox[test:test+batch_size], (batch_size, vox[test:test+batch_size].shape[1],\\\n",
    "                                                                                        vox[test:test+batch_size].shape[2], vox[test:test+batch_size].shape[3], 1)),\\\n",
    "                                                       regress_labels: np.reshape(targets[test:test+batch_size],(batch_size,-1))})\n",
    "            test+=batch_size\n",
    "            print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "            print('epoch=',epoch,'batch=',test,'loss=',l)\n",
    "            test_pred_center[epoch].append([pred,test,l])\n",
    "            all_losses_test[epoch].append([s_loss,c_loss,hc_loss])\n",
    "            np.save('./Windows/test_pred_'+str(epoch), np.array(test_pred_center[epoch]), allow_pickle=True)\n",
    "            np.save('./Windows/test_loss_'+str(epoch), np.array(all_losses_test[epoch]), allow_pickle=True)\n",
    "            if test%50<5:\n",
    "                print('%%%%%%%%%%%%%%%%%%%%%%%%%%AverageLoss%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "                print(np.mean(np.array(all_losses_test[epoch]),axis=0))\n",
    "\n",
    "                #print([l,s_loss,c_loss,hc_loss])\n",
    "                \n",
    "                #print(np.mean(np.array(test_pred_center[epoch]), axis=0)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_losses_train[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_center = {}\n",
    "test_pred_center = {}\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    #values = sess.run(variables_names)\n",
    "    #for k, v in zip(variables_names, values):\n",
    "        #print(\"Variable: \", k)\n",
    "        #print(\"Shape: \", v.shape)\n",
    "    # uncomment this when we want to load the checkpoint model\n",
    "    saver.restore(sess, \"./checkpoints/regression_\"+str(15)+'_'+str(5000)+\".ckpt\")\n",
    "    vox = np.load('./Windows/voxels.npy')\n",
    "    labels = np.load('./Windows/lbh.npy')\n",
    "    assert vox.shape[0]==labels.shape[0]\n",
    "    assert vox[0].shape==(30,45,20)\n",
    "    \n",
    "    for epoch in range(40):\n",
    "        print('**************************************************************************************')\n",
    "        i=0\n",
    "        batch_size=5\n",
    "        pred_center[epoch] = []\n",
    "        while(i<(vox.shape[0]-2000)):\n",
    "            _, l, center = sess.run([train_op, loss, out], feed_dict={voxel_inputs:np.reshape(vox[i:i+batch_size], (batch_size, vox[i:i+batch_size].shape[1],\\\n",
    "                                                                                        vox[i:i+batch_size].shape[2], vox[i:i+batch_size].shape[3], 1)),\\\n",
    "                                                       regress_labels: np.reshape(labels[i:i+batch_size],(batch_size,-1))})\n",
    "            i+=batch_size\n",
    "            print('epoch=',epoch,'batch=',i,'loss=',l)\n",
    "            pred_center[epoch].append([center,i,l])\n",
    "            if i%1000==0:\n",
    "                save_path = saver.save(sess, \"./checkpoints/lbhregression_\"+str(epoch)+'_'+str(i)+\".ckpt\")\n",
    "                print('%%%%%%%%%%%%%%%%%%%%%%%%%%AverageLoss%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "                print(np.mean(np.array(pred_center[epoch]), axis=0)[2])\n",
    "        test=vox.shape[0]-2000\n",
    "        test_pred_center[epoch]=[]\n",
    "        while(test<(vox.shape[0])):\n",
    "            l, center = sess.run([loss, out], feed_dict={voxel_inputs:np.reshape(vox[test:test+batch_size], (batch_size, vox[test:test+batch_size].shape[1],\\\n",
    "                                                                                        vox[test:test+batch_size].shape[2], vox[test:test+batch_size].shape[3], 1)),\\\n",
    "                                                       regress_labels: np.reshape(labels[test:test+batch_size],(batch_size,-1))})\n",
    "            test+=batch_size\n",
    "            print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "            print('epoch=',epoch,'batch=',test,'loss=',l)\n",
    "            test_pred_center[epoch].append([center,test,l])\n",
    "            if test%500<5:\n",
    "                print('%%%%%%%%%%%%%%%%%%%%%%%%%%AverageLoss%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%')\n",
    "                print(np.mean(np.array(test_pred_center[epoch]), axis=0)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#huber loss\n",
    "#gradient clipping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(labels[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(labels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(labels[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
