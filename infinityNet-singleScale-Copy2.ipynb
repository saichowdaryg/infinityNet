{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vamsi/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import importlib\n",
    "from train import provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as cPickle\n",
    "def demo(fname):\n",
    "    #import mayavi.mlab as mlab\n",
    "    #from viz_util import draw_lidar, draw_lidar_simple, draw_gt_boxes3d\n",
    "    with open(fname, 'rb') as input_file:\n",
    "        try:\n",
    "            while True:\n",
    "                yield cPickle.load(input_file)\n",
    "        except EOFError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ba324b1d632>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./kitti/frustum_carpedcyc_train.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-1419cd6e923c>\u001b[0m in \u001b[0;36mdemo\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h = list(demo(\"./kitti/frustum_carpedcyc_train.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list(demo(\"./kitti/frustum_carpedcyc_val.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_POINT=2048\n",
    "TRAIN_DATASET = provider.FrustumDataset(npoints=NUM_POINT, split='train',\\\n",
    "                            rotate_to_center=True, random_flip=False, random_shift=True, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL_DATASET = provider.FrustumDataset(npoints=NUM_POINT, split='val',\\\n",
    "                            rotate_to_center=True, random_flip=False, random_shift=True, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def indices(num, window_len, stride_len, key='train'):\n",
    "    IOU = []\n",
    "    start = []\n",
    "    end = []\n",
    "    labels = []\n",
    "    point_set = []\n",
    "    if key=='train':\n",
    "        ind = np.argsort(TRAIN_DATASET.get_center_view_point_set(num)[:,2])\n",
    "        point_set = (TRAIN_DATASET.get_center_view_point_set(num)[ind])[:,:3]\n",
    "        labels = h[4][num]\n",
    "        labels = labels[ind]\n",
    "    elif key=='val':\n",
    "        ind = np.argsort(VAL_DATASET.get_center_view_point_set(num)[:,2])\n",
    "        point_set = (VAL_DATASET.get_center_view_point_set(num)[ind])[:,:3]\n",
    "        labels = val[4][num]\n",
    "        labels = labels[ind]\n",
    "    j = np.min(point_set,0)[2]\n",
    "    i = np.min(point_set,0)[2]+window_len\n",
    "    idtuple=set()\n",
    "    while i<=np.max(point_set,0)[2]:\n",
    "            max_ind = np.searchsorted(point_set[:,2], i)\n",
    "            min_ind = np.searchsorted(point_set[:,2], j)\n",
    "            if (min_ind,max_ind) not in idtuple and max_ind-min_ind>5:\n",
    "                #print('fgbd')\n",
    "                idtuple.add((min_ind,max_ind))\n",
    "                labels_window = np.zeros_like(point_set[:,0])\n",
    "                labels_window[min_ind:max_ind] = 1.0\n",
    "                union = np.sum(np.logical_or(labels, labels_window))\n",
    "                inter = np.sum(np.logical_and(labels, labels_window))\n",
    "                if (inter*1.0/union) < 0.3 or (inter*1.0/union) >0.7 :\n",
    "                    end.append(max_ind)\n",
    "                    start.append(min_ind)\n",
    "                    IOU.append(inter*1.0/union)\n",
    "            j = j+stride_len\n",
    "            i = j+window_len\n",
    "    IOU = np.array(IOU)>0.7\n",
    "    return np.array(start),np.array(end), IOU\n",
    "\n",
    "a,b,c = indices(0,1.0,0.1)\n",
    "a.shape,b.shape,c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points in each window:2048\n",
    "# number of windows\n",
    "#\n",
    "\n",
    "def get_list(start_idx,end_idx,num_points=1024,ensure_all=True):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    start_idx  : list of indexes of the starting point in each window\n",
    "    end_idx    : list of indees of ending point in each window\n",
    "    NOTES: we used two lists because we ran into issues with tensorflow when we used tuples,\n",
    "           this is a quick hack to avoid that\n",
    "    num_points : number of points to be sampled from i.e number of points in each window\n",
    "    ensure_all : in the case that there are fewwer than required points ensures all points are picked atleast once if selected as True\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "    out:  an array of size \n",
    "          \n",
    "          num_windowsxnum_pointsx1\n",
    "          \n",
    "          this will be used inside tensorflow to divide into windows \n",
    "          after we pass the points trough pointnet\n",
    "    \"\"\"\n",
    "    \n",
    "    out= []\n",
    "    for i in range(np.shape(start_idx)[0]):\n",
    "        \n",
    "        if end_idx[i]-start_idx[i]+1<num_points:\n",
    "            indexes=np.arange(start_idx[i],end_idx[i]+1)\n",
    "            indexes2=np.random.choice(indexes, num_points-np.shape(indexes)[0],replace=True)\n",
    "            indexes=np.concatenate((indexes,indexes2))\n",
    "        else:\n",
    "            indexes=np.random.choice(np.arange(start_idx[i],end_idx[i]+1),num_points,replace=False)\n",
    "        out.append(np.reshape(indexes,(-1,1)))\n",
    "    \n",
    "    return np.array(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "#for i in range(len(h[5])):\n",
    "s,e,l=indices(0, 0.9, 0.1)\n",
    "\n",
    "s1, e1, l1 = indices(0, 3.0, 0.3)\n",
    "\n",
    "ind= get_list(s,e, num_points=512)\n",
    "\n",
    "ind1 = get_list(s1,e1, num_points=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ind.shape)\n",
    "print(ind1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_util #for conv2d\n",
    "from model_util import NUM_HEADING_BIN, NUM_SIZE_CLUSTER, NUM_OBJECT_POINT\n",
    "from model_util import point_cloud_masking, get_center_regression_net\n",
    "from model_util import placeholder_inputs, parse_output_to_tensors, get_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def focal_loss(prediction,target,gamma,alpha):\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ailias/Focal-Loss-implement-on-Tensorflow/blob/master/focal_loss.py\n",
    "\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "def focal_loss(sigmoid_p, target_tensor, weights=None, alpha=0.99, gamma=2):\n",
    "    r\"\"\"Compute focal loss for predictions.\n",
    "        Multi-labels Focal loss formula:\n",
    "            FL = -alpha * (z-p)^gamma * log(p) -(1-alpha) * p^gamma * log\n",
    "            (1-p)\n",
    "                 ,which alpha = 0.25, gamma = 2, p = sigmoid(x), z = target_tensor.\n",
    "    Args:\n",
    "     prediction_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing the predicted logits for each class\n",
    "     target_tensor: A float tensor of shape [batch_size, num_anchors,\n",
    "        num_classes] representing one-hot encoded classification targets\n",
    "     weights: A float tensor of shape [batch_size, num_anchors]\n",
    "     alpha: A scalar tensor for focal loss alpha hyper-parameter\n",
    "     gamma: A scalar tensor for focal loss gamma hyper-parameter\n",
    "    Returns:\n",
    "        loss: A (scalar) tensor representing the value of the loss function\n",
    "    \"\"\"\n",
    "    #sigmoid_p = tf.nn.sigmoid(prediction_tensor)\n",
    "    zeros = array_ops.zeros_like(target_tensor, dtype=target_tensor.dtype)\n",
    "    \n",
    "    # For poitive prediction, only need consider front part loss, back part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so poitive coefficient = z - p.\n",
    "    pos_p_sub = array_ops.where(target_tensor > zeros, target_tensor - sigmoid_p, zeros)\n",
    "    \n",
    "    # For negative prediction, only need consider back part loss, front part is 0;\n",
    "    # target_tensor > zeros <=> z=1, so negative coefficient = 0.\n",
    "    neg_p_sub = array_ops.where(target_tensor > zeros, zeros, sigmoid_p)\n",
    "    per_entry_cross_ent = - alpha * (pos_p_sub ** gamma) * tf.log(tf.clip_by_value(sigmoid_p, 1e-8, 1.0)) \\\n",
    "                          - (1.0 - alpha) * (neg_p_sub ** gamma) * tf.log(tf.clip_by_value(1.0 - sigmoid_p, 1e-8, 1.0))\n",
    "    return tf.reduce_sum(per_entry_cross_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classification_net(point_cloud, label_out_5, label_out_1, label_out_2, label_out_3, label_out_4, idx5, idx1, idx2, idx3, idx4, \n",
    "#                             is_training=True, bn_decay=True):\n",
    "def classification_net(point_cloud, label_out_5,  idx5, \n",
    "                            is_training=True, bn_decay=True):\n",
    "    '''\n",
    "    3D PointNet network.\n",
    "    param:\n",
    "    \n",
    "    point_cloud: TF tensor in shape (1,N,4)\n",
    "                 frustum point clouds with XYZ and intensity in point channels\n",
    "                 XYZs are in frustum coordinate\n",
    "    label_out: TF tensor in shape (1,1)\n",
    "            length-1 vector indicating the classification as car or background\n",
    "    \n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    \n",
    "    is_training: TF boolean scalar\n",
    "    bn_decay: TF float scalar\n",
    "    end_points: dict                 SHOULD CHANGE THIS\n",
    "    Output:                                         \n",
    "    logits: TF tensor in shape (B,N,2), scores for bkg/clutter and object\n",
    "    end_points: dict\n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    '''\n",
    "    \n",
    "    batch_size = point_cloud.get_shape()[0].value #1 in our case\n",
    "    num_point = point_cloud.get_shape()[1].value  #N in our case\n",
    "  \n",
    "    net = tf.expand_dims(point_cloud, 2)\n",
    "    #print(net.get_shape())\n",
    "\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv1', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv2', bn_decay=bn_decay)\n",
    "    point_feat = tf_util.conv2d(net, 64, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv3', bn_decay=bn_decay)\n",
    "    net = tf_util.conv2d(point_feat, 128, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv4', bn_decay=bn_decay)\n",
    "    features = tf_util.conv2d(net, 1024, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='conv5', bn_decay=bn_decay)\n",
    "    \n",
    "    features=tf.squeeze(features,axis=2)\n",
    "    features=tf.squeeze(features,axis=0)   \n",
    "\n",
    "    #features=tf.placeholder(tf.float32,[1,1024,4])\n",
    "\n",
    "    def fn(x):\n",
    "        return tf.gather_nd(features,x)\n",
    "    \n",
    "    windows_5 =tf.map_fn(fn,idx5,dtype=tf.float32)\n",
    "#     windows_1 = tf.map_fn(fn,idx1,dtype=tf.float32)\n",
    "#     windows_2 = tf.map_fn(fn,idx2,dtype=tf.float32)\n",
    "#     windows_3 = tf.map_fn(fn,idx3,dtype=tf.float32)\n",
    "#     windows_4 = tf.map_fn(fn,idx4,dtype=tf.float32)\n",
    "    \n",
    "    windows_5_1024 = tf.reduce_max(windows_5,axis=1)\n",
    "#     windows_1_1024 = tf.reduce_max(windows_1,axis=1)\n",
    "#     windows_2_1024 = tf.reduce_max(windows_2,axis=1)\n",
    "#     windows_3_1024 = tf.reduce_max(windows_3,axis=1)\n",
    "#     windows_4_1024 = tf.reduce_max(windows_4,axis=1)\n",
    "    \n",
    "    \n",
    "    winex_5 =  tf.reshape(windows_5_1024,shape=[-1,1,1,1024])\n",
    "#     winex_1 =  tf.reshape(windows_1_1024,shape=[-1,1,1,1024])\n",
    "#     winex_2 =  tf.reshape(windows_2_1024,shape=[-1,1,1,1024])\n",
    "#     winex_3 =  tf.reshape(windows_3_1024,shape=[-1,1,1,1024])\n",
    "#     winex_4 =  tf.reshape(windows_4_1024,shape=[-1,1,1,1024])\n",
    "   \n",
    "    winex = tf.concat([winex_5], axis=0)\n",
    "    label_out = tf.concat([label_out_5],axis=0)\n",
    "\n",
    "    \n",
    "    \n",
    "#     winex = tf.concat([winex_5, winex_1, winex_2, winex_3, winex_4], axis=0)\n",
    "#     label_out = tf.concat([label_out_5,label_out_1, label_out_2, label_out_3, label_out_4],axis=0)\n",
    "\n",
    "    #classification head\n",
    "    d1 = tf_util.conv2d(winex, 512, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='dense1', bn_decay=bn_decay)\n",
    "    d2 = tf_util.conv2d(d1, 256, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='dense2', bn_decay=bn_decay)\n",
    "    d2=tf.layers.dropout(d2,rate=0.5, training=is_training, name='drop1')\n",
    "    \n",
    "    logits = tf_util.conv2d(d2, 1, [1,1],\n",
    "                             padding='VALID', stride=[1,1],\n",
    "                             bn=True, is_training=is_training,\n",
    "                             scope='out', bn_decay=bn_decay, activation_fn=None)\n",
    "    \n",
    "    logits = tf.squeeze(logits)\n",
    "    logits=tf.reshape(tf.squeeze(logits),(1,-1))\n",
    "    sig = tf.sigmoid(logits)\n",
    "\n",
    "    #sig = tf.reshape(tf.squeeze(sig),(-1,1))\n",
    "    y=tf.reshape(sig,[-1])\n",
    "    \n",
    "    max_ind = tf.argmax(y)\n",
    "    \n",
    "    tp = tf.equal(label_out[max_ind], 1)\n",
    "    \n",
    "    \n",
    "    differentiable_round = tf.maximum(y-0.499,0)\n",
    "    differentiable_round = differentiable_round * 10000\n",
    "    y_pred = tf.minimum(differentiable_round, 1)\n",
    "    #y_pred=tf.round(y)\n",
    "    \n",
    "    true_pos  = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(label_out,tf.bool),tf.cast(y_pred,tf.bool)),tf.float32))\n",
    "    \n",
    "    false_pos = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(tf.cast(label_out,tf.bool)),tf.cast(y_pred,tf.bool)),tf.float32))\n",
    "    true_neg  = tf.reduce_sum(tf.cast(tf.logical_not(tf.logical_or(tf.cast(label_out, tf.bool),\\\n",
    "                                                                   tf.cast(y_pred, tf.bool))),tf.float32))\n",
    "    false_neg = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not\\\n",
    "                                                     (tf.cast(y_pred,tf.bool)),tf.cast(label_out,tf.bool)),tf.float32))\n",
    "    return sig,logits, label_out,true_pos,false_pos,true_neg,false_neg, tp, max_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch=tf.reshape(sig,[-1])\n",
    "# ch=tf.round(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.device('/device:GPU:0'):\n",
    "    num_points_5 =512\n",
    "#     num_points_1 = 512\n",
    "#     num_points_2 = 512\n",
    "#     num_points_3 = 512\n",
    "#     num_points_4 = 512\n",
    "    \n",
    "    inputs=tf.placeholder(tf.float32,[None,2048,4])#put placeholder\n",
    "\n",
    "    idx_5 =tf.placeholder(tf.int32,[None,num_points_5,1])\n",
    "\n",
    "#     idx_1 = tf.placeholder(tf.int32,[None,num_points_1,1])\n",
    "    \n",
    "#     idx_2 = tf.placeholder(tf.int32,[None,num_points_2,1])\n",
    "\n",
    "#     idx_3 = tf.placeholder(tf.int32,[None,num_points_3,1])\n",
    "    \n",
    "#     idx_4 = tf.placeholder(tf.int32,[None,num_points_4,1])\n",
    "\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "\n",
    "    label_out_5 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_1 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_2 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_3 = tf.placeholder(tf.int32, [None])\n",
    "#     label_out_4 = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    sig,logits, label_out,tp,fp,tn,fn, tp_num, maxi = classification_net(inputs, label_out_5, \\\n",
    "                                         idx_5, is_training)\n",
    "    \n",
    "    \n",
    "#     sig,logits, label_out,tp,fp,tn,fn = classification_net(inputs, label_out_5, label_out_1,label_out_2, label_out_3, label_out_4, \\\n",
    "#                                          idx_5, idx_1, idx_2, idx_3, idx_4, is_training)\n",
    "    \n",
    "    func_fl = focal_loss(tf.squeeze(sig),tf.squeeze(tf.cast(label_out, tf.float32)), gamma=0.75, alpha=0.25)\n",
    "    print(label_out.get_shape())\n",
    "#     loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#     labels=tf.reshape(tf.cast(label_out,tf.float32),(-1,1)),\n",
    "#     logits=tf.reshape(logits,(-1,1))))\n",
    "\n",
    "    loss = tf.reduce_mean(func_fl)\n",
    "#     tpr=tf.divide(tf.cast(tp,tf.float32),tf.add(tp,fp))\n",
    "#     tnr=tf.divide(tf.cast(tn,tf.float32),tf.add(tn,fn))\n",
    "#     loss=tf.reduce_sum(tf.constant(1.0)-tf.scalar_mul(0.5,tf.add(tpr,tnr)))\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(1e-5)\n",
    "    #train_step=optimizer.minimize(loss)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "from random import shuffle\n",
    "#logging.basicConfig(filename='run_1.log',level=logging.DEBUG)\n",
    "from tensorflow.python import debug as tf_debug\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "trix=[cc for cc in range(len(h[5]))]\n",
    "shuffle(trix)\n",
    "prob = []\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "    #sess=tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    variables_names = [v.name for v in tf.trainable_variables()]\n",
    "    values = sess.run(variables_names)\n",
    "    for k, v in zip(variables_names, values):\n",
    "        print \"Variable: \", k\n",
    "        print \"Shape: \", v.shape\n",
    "    # uncomment this when we want to load the checkpoint model\n",
    "    saver.restore(sess, \"./checkpoints/model_Gamma0.75_14_8666.ckpt\")\n",
    "    \n",
    "    \n",
    "    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "    k = 0\n",
    "    print(len(val[5]))\n",
    "    Vtotal_tp=0\n",
    "    Vtotal_fp=0\n",
    "    Vtotal_tn=0\n",
    "    Vtotal_fn=0\n",
    "    Vtotal_tpnums = 0\n",
    "    Vtotal_fpnums = 0\n",
    "    Vtotal_fnnums = 0\n",
    "    while k<len(val[5]):\n",
    "    #todo reformat code to have lists instead of many variable\n",
    "\n",
    "\n",
    "        if val[5][k]=='Pedestrian':\n",
    "\n",
    "            index = np.argsort(VAL_DATASET.get_center_view_point_set(k)[:,2])\n",
    "            point_set = VAL_DATASET.get_center_view_point_set(k)[index]\n",
    "\n",
    "            if point_set.shape[0]<2048:\n",
    "                    indexes=np.arange(np.shape(point_set)[0])\n",
    "                    indexes2=np.random.choice(indexes, 2048-np.shape(indexes)[0],replace=True)\n",
    "                    indexes=np.concatenate((indexes,indexes2))\n",
    "            else:\n",
    "                    indexes=np.random.choice(np.arange(np.shape(point_set)[0]),2048,replace=False)\n",
    "\n",
    "            point_set = np.reshape(point_set[indexes], (1, 2048, 4))\n",
    "\n",
    "            s5,e5,l5 = indices(k, 0.9, 0.1, 'val')\n",
    "\n",
    "#                 s1, e1, l1 = indices(k, 1.1, 0.2, 'val')\n",
    "\n",
    "#                 s2, e2, l2 = indices(k, 1.3, 0.2, 'val')\n",
    "\n",
    "#                 s3, e3, l3 = indices(k, 0.7, 0.1, 'val')\n",
    "\n",
    "#                 s4, e4, l4 = indices(k, 0.5, 0.1, 'val')\n",
    "\n",
    "            if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "#                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "\n",
    "#                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "\n",
    "                print(Vtotal_tp)\n",
    "\n",
    "                x,tpos,fpos,tneg,fneg, tp_nums, maxind =sess.run([loss,tp,fp,tn,fn, tp_num, maxi],feed_dict={idx_5:ind_5, \\\n",
    "                                                           label_out_5: l5.astype(np.int32), inputs:point_set, is_training: True})\n",
    "                print(Vtotal_tp)\n",
    "                Vtotal_tp +=tpos\n",
    "                Vtotal_fp +=fpos\n",
    "                Vtotal_tn +=tneg\n",
    "                Vtotal_fn +=fneg\n",
    "                Vtotal_tpnums+= tp_nums\n",
    "                Vtotal_fpnums+= (1-tp_nums)\n",
    "                Vtotal_fnnums+= (1-tp_nums)\n",
    "                \n",
    "                print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "                \n",
    "                print('Debug :',maxind, l5[maxind], tp_nums)\n",
    "                print('Epoch ', j , 'Loss for the current example ',k,' = ', x)\n",
    "                #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                print('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                      ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "\n",
    "                print('Single metrics: Precision = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fpnums),\\\n",
    "                      ' Recall = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fnnums))\n",
    "                #logging.info('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                #      ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for j in range(6,15):   \n",
    "        total_tp=0\n",
    "        total_fp=0\n",
    "        total_tn=0\n",
    "        total_fn=0\n",
    "        total_tpnums = 0\n",
    "        total_fpnums = 0\n",
    "        total_fnnums = 0\n",
    "        \n",
    "        \n",
    "        print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\"))\n",
    "        \n",
    "        print('Epoch Number........................................',j)\n",
    "        pedc=0\n",
    "        \n",
    "        for i in trix:\n",
    "            \n",
    "        #todo reformat code to have lists instead of many variable\n",
    "            if h[5][i]=='Pedestrian':\n",
    "                pedc+=1\n",
    "                if pedc%500==0:\n",
    "                    total_tp=0\n",
    "                    total_fp=0\n",
    "                    total_tn=0\n",
    "                    total_fn=0\n",
    "                index = np.argsort(TRAIN_DATASET.get_center_view_point_set(i)[:,2])\n",
    "                point_set = TRAIN_DATASET.get_center_view_point_set(i)[index]\n",
    "\n",
    "                if point_set.shape[0]<2048:\n",
    "                        indexes=np.arange(np.shape(point_set)[0])\n",
    "                        indexes2=np.random.choice(indexes, 2048-np.shape(indexes)[0],replace=True)\n",
    "                        indexes=np.concatenate((indexes,indexes2))\n",
    "                else:\n",
    "                        indexes=np.random.choice(np.arange(np.shape(point_set)[0]),2048,replace=False)\n",
    "\n",
    "                point_set = np.reshape(point_set[indexes], (1, 2048, 4))\n",
    "                \n",
    "                \n",
    "                \n",
    "                s5,e5,l5 = indices(i, 0.9, 0.1)\n",
    "\n",
    "#                 s1, e1, l1 = indices(i, 1.1, 0.2)\n",
    "\n",
    "#                 s2, e2, l2 = indices(i, 1.3, 0.2)\n",
    "\n",
    "#                 s3, e3, l3 = indices(i, 0.7, 0.1)\n",
    "\n",
    "#                 s4, e4, l4 = indices(i, 0.5, 0.1)\n",
    "                \n",
    "                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "#                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "#                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "                        \n",
    "                    _, x,tpos,fpos,tneg,fneg, tpnums =sess.run([train_op, loss,tp,fp,tn,fn, tp_num],feed_dict={idx_5:ind_5,\\\n",
    "                                                               label_out_5: l5.astype(np.int32),\\\n",
    "                                                               inputs:point_set, is_training: True})\n",
    "                    total_tp+=tpos\n",
    "                    total_fp+=fpos\n",
    "                    total_tn+=tneg\n",
    "                    total_fn+=fneg\n",
    "                    total_tpnums+= tpnums\n",
    "                    total_fpnums+= (1-tpnums)\n",
    "                    total_fnnums+= (1-tpnums)\n",
    "                    print('----------------------------------------------------------------------------------')\n",
    "                    print('Epoch ', j , 'Loss for the current example ',i,' = ', x,'pedc = ',pedc)\n",
    "                    #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                    print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                    #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                    #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                    print('Cumulative metrics: Precision = ',(total_tp*1.0)/(total_tp+total_fp),\\\n",
    "                          ' Recall = ',(total_tp*1.0)/(total_tp+total_fn))\n",
    "                    #logging.info('Cumulative metrics: Precision = ',(total_tp*1.0)/(total_tp+total_fp),\\\n",
    "                    #      ' Recall = ',(total_tp*1.0)/(total_tp+total_fn))\n",
    "                    print('Single metrics: Precision = ',(total_tpnums*1.0)/(total_tpnums+total_fpnums),\\\n",
    "                          ' Recall = ',(total_tpnums*1.0)/(total_tpnums+total_fnnums))\n",
    "                    print('----------------------------------------------------------------------------------')\n",
    "                    \n",
    "                    if(pedc%5000==0):\n",
    "                        save_path = saver.save(sess, \"./checkpoints/model_Gamma0.75_\"+str(j)+'_'+str(i)+\".ckpt\")\n",
    "                    if pedc%12000==11999:\n",
    "                        print('***************************************************************************************************')\n",
    "                        k = 0\n",
    "                        print(len(val[5]))\n",
    "                        Vtotal_tp=0\n",
    "                        Vtotal_fp=0\n",
    "                        Vtotal_tn=0\n",
    "                        Vtotal_fn=0\n",
    "                        while k<len(val[5]):\n",
    "                        #todo reformat code to have lists instead of many variable\n",
    "                            \n",
    "                        \n",
    "                            if val[5][k]=='Pedestrian':\n",
    "                                \n",
    "                                index = np.argsort(VAL_DATASET.get_center_view_point_set(k)[:,2])\n",
    "                                point_set = VAL_DATASET.get_center_view_point_set(k)[index]\n",
    "                                \n",
    "                                if point_set.shape[0]<2048:\n",
    "                                        indexes=np.arange(np.shape(point_set)[0])\n",
    "                                        indexes2=np.random.choice(indexes, 2048-np.shape(indexes)[0],replace=True)\n",
    "                                        indexes=np.concatenate((indexes,indexes2))\n",
    "                                else:\n",
    "                                        indexes=np.random.choice(np.arange(np.shape(point_set)[0]),2048,replace=False)\n",
    "                                \n",
    "                                point_set = np.reshape(point_set[indexes], (1, 2048, 4))\n",
    "\n",
    "                                s5,e5,l5 = indices(k, 0.9, 0.1, 'val')\n",
    "\n",
    "#                                 s1, e1, l1 = indices(k, 1.1, 0.2, 'val')\n",
    "\n",
    "#                                 s2, e2, l2 = indices(k, 1.3, 0.2, 'val')\n",
    "\n",
    "#                                 s3, e3, l3 = indices(k, 0.7, 0.1, 'val')\n",
    "\n",
    "#                                 s4, e4, l4 = indices(k, 0.5, 0.1, 'val')\n",
    "                                \n",
    "                                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "                                    \n",
    "                                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "#                                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "\n",
    "#                                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "                                    \n",
    "                                    print(Vtotal_tp)\n",
    "\n",
    "                                    x,tpos,fpos,tneg,fneg =sess.run([loss,tp,fp,tn,fn],feed_dict={idx_5:ind_5, \\\n",
    "                                                                               label_out_5: l5.astype(np.int32), inputs:point_set, is_training : True})\n",
    "                                    \n",
    "                                    print(Vtotal_tp)\n",
    "                                    Vtotal_tp +=tpos\n",
    "                                    Vtotal_fp +=fpos\n",
    "                                    Vtotal_tn +=tneg\n",
    "                                    Vtotal_fn +=fneg\n",
    "                                    print('***************************************************************************************************')\n",
    "                                    print('Epoch ', j , 'Loss for the current example ',k,' = ', x)\n",
    "                                    #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                                    print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                                    #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                                    #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                                    print('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                                          ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                                    #logging.info('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                                    #      ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                                    print('***************************************************************************************************')\n",
    "                                    \n",
    "                            k += 1\n",
    "\n",
    "                        \n",
    "                        \n",
    "\n",
    "                #print(c)\n",
    "                #print(n)\n",
    "            #i += 1\n",
    "        #-----------------------------------------------------------------------------------------------#\n",
    "        print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "        k = 0\n",
    "        print(len(val[5]))\n",
    "        Vtotal_tp=0\n",
    "        Vtotal_fp=0\n",
    "        Vtotal_tn=0\n",
    "        Vtotal_fn=0\n",
    "        Vtotal_tpnums = 0\n",
    "        Vtotal_fpnums = 0\n",
    "        Vtotal_fnnums = 0\n",
    "        while k<len(val[5]):\n",
    "        #todo reformat code to have lists instead of many variable\n",
    "\n",
    "\n",
    "            if val[5][k]=='Pedestrian':\n",
    "\n",
    "                index = np.argsort(VAL_DATASET.get_center_view_point_set(k)[:,2])\n",
    "                point_set = VAL_DATASET.get_center_view_point_set(k)[index]\n",
    "\n",
    "                if point_set.shape[0]<2048:\n",
    "                        indexes=np.arange(np.shape(point_set)[0])\n",
    "                        indexes2=np.random.choice(indexes, 2048-np.shape(indexes)[0],replace=True)\n",
    "                        indexes=np.concatenate((indexes,indexes2))\n",
    "                else:\n",
    "                        indexes=np.random.choice(np.arange(np.shape(point_set)[0]),2048,replace=False)\n",
    "\n",
    "                point_set = np.reshape(point_set[indexes], (1, 2048, 4))\n",
    "\n",
    "                s5,e5,l5 = indices(k, 0.9, 0.1, 'val')\n",
    "\n",
    "#                 s1, e1, l1 = indices(k, 1.1, 0.2, 'val')\n",
    "\n",
    "#                 s2, e2, l2 = indices(k, 1.3, 0.2, 'val')\n",
    "\n",
    "#                 s3, e3, l3 = indices(k, 0.7, 0.1, 'val')\n",
    "\n",
    "#                 s4, e4, l4 = indices(k, 0.5, 0.1, 'val')\n",
    "\n",
    "                if s5.size :#and s2.size and s3.size and s4.size and s1.size:\n",
    "\n",
    "                    ind_5 = get_list(s5,e5, num_points=512)\n",
    "\n",
    "#                     ind_1 = get_list(s1,e1, num_points=512)\n",
    "\n",
    "#                     ind_2 = get_list(s2,e2, num_points=512)\n",
    "\n",
    "#                     ind_3 = get_list(s3,e3, num_points=512)\n",
    "\n",
    "#                     ind_4 = get_list(s4,e4, num_points=512)\n",
    "\n",
    "#                     t_windows=s1.shape[0]+s2.shape[0]+s3.shape[0]+s4.shape[0]+s5.shape[0]\n",
    "\n",
    "                    print(Vtotal_tp)\n",
    "\n",
    "                    x,tpos,fpos,tneg,fneg, tp_nums =sess.run([loss,tp,fp,tn,fn, tp_num],feed_dict={idx_5:ind_5, \\\n",
    "                                                               label_out_5: l5.astype(np.int32), inputs:point_set, is_training: True})\n",
    "                    print(Vtotal_tp)\n",
    "                    Vtotal_tp +=tpos\n",
    "                    Vtotal_fp +=fpos\n",
    "                    Vtotal_tn +=tneg\n",
    "                    Vtotal_fn +=fneg\n",
    "                    Vtotal_tpnums+= tpnums\n",
    "                    Vtotal_fpnums+= (1-tpnums)\n",
    "                    Vtotal_fnnums+= (1-tpnums)\n",
    "                    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "                    print('Epoch ', j , 'Loss for the current example ',k,' = ', x)\n",
    "                    #logging.info('Loss for the current example ',i,' = ', x)\n",
    "                    print('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)\n",
    "                    #logging.info('Metrics for current example: TP=',tpos,' FP=',fpos,' TN=',tneg,' FN=',fneg)  # will not print anything\n",
    "                    #print('Cumulative metrics: TP=',total_tp,' FP=',total_fp,' TN=',total_tn,' FN=',total_fn)\n",
    "                    print('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                          ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                    \n",
    "                    print('Single metrics: Precision = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fpnums),\\\n",
    "                          ' Recall = ',(Vtotal_tpnums*1.0)/(Vtotal_tpnums+Vtotal_fnnums))\n",
    "                    #logging.info('Cumulative metrics: Precision = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fp),\\\n",
    "                    #      ' Recall = ',(Vtotal_tp*1.0)/(Vtotal_tp+Vtotal_fn))\n",
    "                    print('&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')\n",
    "\n",
    "            k += 1\n",
    "\n",
    "        #----------------------------------------------------------------------------------------------#\n",
    "#         prob = sess.run([sig],feed_dict={idx_5:ind_5, idx_1:ind_1, idx_2:ind_2, idx_3:ind_3, idx_4:ind_4,\\\n",
    "#                                                        label_out_5: l5.astype(np.int32), label_out_1: l1.astype(np.int32),\\\n",
    "#                                                        label_out_2: l2.astype(np.int32), label_out_3: l3.astype(np.int32),\\\n",
    "#                                                        label_out_4: l4.astype(np.int32), inputs:point_set,  is_training: False})\n",
    "#         print(prob)\n",
    "        print(ind_5)\n",
    "        print(label_out_5)\n",
    "        prob = sess.run([sig],feed_dict={idx_5:ind_5,  \\\n",
    "                                                       label_out_5: l5.astype(np.int32), inputs:point_set,  is_training: False})\n",
    "        print(prob)\n",
    "print(datetime.datetime.now().strftime(\"%a, %d %B %Y %I:%M:%S\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_net(point_cloud, label_out_5, label_out_1, label_out_2, label_out_3, label_out_4, idx5, idx1, idx2, idx3, idx4, \n",
    "                            is_training=True, bn_decay=None):\n",
    "    '''\n",
    "    3D PointNet network.\n",
    "    param:\n",
    "    \n",
    "    point_cloud: TF tensor in shape (1,N,4)\n",
    "                 frustum point clouds with XYZ and intensity in point channels\n",
    "                 XYZs are in frustum coordinate\n",
    "    label_out: TF tensor in shape (1,1)\n",
    "            length-1 vector indicating the classification as car or background\n",
    "    \n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    \n",
    "    is_training: TF boolean scalar\n",
    "    bn_decay: TF float scalar\n",
    "    end_points: dict                 SHOULD CHANGE THIS\n",
    "    Output:                                         \n",
    "    logits: TF tensor in shape (B,N,2), scores for bkg/clutter and object\n",
    "    end_points: dict\n",
    "    ??????????????????????????????????????????????????????????????????????????????????????\n",
    "    '''\n",
    "    \n",
    "    batch_size = point_cloud.get_shape()[0].value #1 in our case\n",
    "    num_point = point_cloud.get_shape()[1].value  #N in our case\n",
    "  \n",
    "    net = tf.expand_dims(point_cloud, 2)\n",
    "    #print(net.get_shape())\n",
    "    print('AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')\n",
    "    net = tf.layers.conv2d(net, 64, [1,1], strides=[1,1], padding='valid', name='conv1', activation= tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    net = tf.layers.batch_normalization(net, training=is_training, name='bn1')\n",
    "    \n",
    "    net = tf.layers.conv2d(net, 64, [1,1], strides=[1,1], padding='valid', name='conv2', activation= tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    net = tf.layers.batch_normalization(net, training=is_training, name='bn2')\n",
    "    \n",
    "    net = tf.layers.conv2d(net, 64, [1,1], strides=[1,1], padding='valid', name='conv3', activation= tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    net = tf.layers.batch_normalization(net, training=is_training, name='bn3')\n",
    "    \n",
    "    net = tf.layers.conv2d(net, 128, [1,1], strides=[1,1], padding='valid', name='conv4', activation= tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    net = tf.layers.batch_normalization(net, training=is_training, name='bn4')\n",
    "    \n",
    "    net = tf.layers.conv2d(net, 1024, [1,1], strides=[1,1], padding='valid', name='conv5', activation= tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    print('BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB')\n",
    "    features = tf.layers.batch_normalization(net, training=is_training, name='bn5')   \n",
    "\n",
    "    \n",
    "    features=tf.squeeze(features,axis=2)\n",
    "    features=tf.squeeze(features,axis=0)   \n",
    "\n",
    "    #features=tf.placeholder(tf.float32,[1,1024,4])\n",
    "\n",
    "    def fn(x):\n",
    "        return tf.gather_nd(features,x)\n",
    "    \n",
    "    windows_5 =tf.map_fn(fn,idx5,dtype=tf.float32)\n",
    "    windows_1 = tf.map_fn(fn,idx1,dtype=tf.float32)\n",
    "    windows_2 = tf.map_fn(fn,idx2,dtype=tf.float32)\n",
    "    windows_3 = tf.map_fn(fn,idx3,dtype=tf.float32)\n",
    "    windows_4 = tf.map_fn(fn,idx4,dtype=tf.float32)\n",
    "    \n",
    "    windows_5_1024 = tf.reduce_max(windows_5,axis=1)\n",
    "    windows_1_1024 = tf.reduce_max(windows_1,axis=1)\n",
    "    windows_2_1024 = tf.reduce_max(windows_2,axis=1)\n",
    "    windows_3_1024 = tf.reduce_max(windows_3,axis=1)\n",
    "    windows_4_1024 = tf.reduce_max(windows_4,axis=1)\n",
    "    \n",
    "    \n",
    "    winex_5 =  tf.reshape(windows_5_1024,shape=[-1,1,1,1024])\n",
    "    winex_1 =  tf.reshape(windows_1_1024,shape=[-1,1,1,1024])\n",
    "    winex_2 =  tf.reshape(windows_2_1024,shape=[-1,1,1,1024])\n",
    "    winex_3 =  tf.reshape(windows_3_1024,shape=[-1,1,1,1024])\n",
    "    winex_4 =  tf.reshape(windows_4_1024,shape=[-1,1,1,1024])\n",
    "    \n",
    "    winex = tf.concat([winex_5, winex_1, winex_2, winex_3, winex_4], axis=0)\n",
    "    label_out = tf.concat([label_out_5,label_out_1, label_out_2, label_out_3, label_out_4],axis=0)\n",
    "    print('CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC')\n",
    "    conv = tf.contrib.layers.fully_connected(winex, 512, scope='dense1')\n",
    "\n",
    "    conv = tf.layers.batch_normalization(conv, training=is_training, name='bn6')\n",
    "\n",
    "    conv = tf.contrib.layers.fully_connected(conv, 128, scope='dense2')\n",
    "\n",
    "    conv = tf.layers.dropout(conv,rate=0.5, training=is_training, name='drop1')\n",
    "    \n",
    "    conv = tf.layers.batch_normalization(conv, training=is_training, name='bn7')\n",
    "    print('DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD')\n",
    "    logits = tf.contrib.layers.fully_connected(conv, 1, scope='dense3')\n",
    "    \n",
    "    logits = tf.squeeze(logits)\n",
    "    logits = tf.reshape(tf.squeeze(logits),(-1,1))\n",
    "    sig = tf.sigmoid(logits)\n",
    "    #sig = tf.reshape(tf.squeeze(sig),(-1,1))\n",
    "    y=tf.reshape(sig,[-1])\n",
    "    y_pred=tf.round(y)\n",
    "    print('EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE')\n",
    "    true_pos  = tf.reduce_sum(tf.cast(tf.logical_and(tf.cast(label_out,tf.bool),tf.cast(y_pred,tf.bool)),tf.int32))\n",
    "    \n",
    "    false_pos = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not(tf.cast(label_out,tf.bool)),tf.cast(y_pred,tf.bool)),tf.int32))\n",
    "    true_neg  = tf.reduce_sum(tf.cast(tf.logical_not(tf.logical_or(tf.cast(label_out, tf.bool),\\\n",
    "                                                                   tf.cast(y_pred, tf.bool))),tf.int32))\n",
    "    false_neg = tf.reduce_sum(tf.cast(tf.logical_and(tf.logical_not\\\n",
    "                                                     (tf.cast(y_pred,tf.bool)),tf.cast(label_out,tf.bool)),tf.int32))\n",
    "    print('FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF')\n",
    "    return sig,logits, label_out,true_pos,false_pos,true_neg,false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(l5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features=classification_net(inputs,tf.ones(32,1),tf.constant(True),None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=tf.squeeze(features,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=tf.squeeze(features,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a=np.reshape(np.array([i for i in range(1,61)],dtype=np.float32),(12,5))\n",
    "\n",
    "num_points=512\n",
    "\n",
    "num_points1 = 1024 \n",
    "#features=tf.placeholder(tf.float32,[1,1024,4])\n",
    "idx=tf.placeholder(tf.int32,[None,num_points,1])\n",
    "\n",
    "idx1=tf.placeholder(tf.int32,[None,num_points1,1])\n",
    "\n",
    "label_out = tf.placeholder(tf.int32, [None])\n",
    "label_out1 = tf.placeholder(tf.int32, [None])\n",
    "def fn(x):\n",
    "    return tf.gather_nd(features,x)\n",
    "windows=tf.map_fn(fn,idx,dtype=tf.float32)\n",
    "windows_1 = tf.map_fn(fn,idx1,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_1024=tf.reduce_max(windows,axis=1)\n",
    "windows_1_1024 = tf.reduce_max(windows_1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_1_1024.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_training=tf.constant(True)\n",
    "bn_decay=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winex_1.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winex=tf.reshape(windows_1024,shape=[-1,1,1,1024])\n",
    "winex_1 =  tf.reshape(windows_1_1024,shape=[-1,1,1,1024])\n",
    "winex.get_shape()\n",
    "winex = tf.concat([winex, winex_1], axis=0)\n",
    "label_out2 = tf.concat([label_out,label_out1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification head\n",
    "d1 = tf_util.conv2d(winex, 512, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='dense1', bn_decay=bn_decay)\n",
    "d2 = tf_util.conv2d(d1, 256, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='dense2', bn_decay=bn_decay)\n",
    "logits = tf_util.conv2d(d2, 1, [1,1],\n",
    "                         padding='VALID', stride=[1,1],\n",
    "                         bn=True, is_training=is_training,\n",
    "                         scope='out', bn_decay=bn_decay, activation_fn=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.squeeze(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## focal loss\n",
    "sig = tf.sigmoid(logits)\n",
    "\n",
    "sig = tf.reshape(tf.squeeze(sig),(-1,1))\n",
    "\n",
    "func_fl = focal_loss(label_out2,tf.concat([1.0-sig, sig],1), gamma=1.0, alpha=0.01)\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(func_fl)\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(5*1e-3)\n",
    "\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    index = np.argsort(TRAIN_DATASET.get_center_view_point_set(0)[:,2])\n",
    "    point_set = TRAIN_DATASET.get_center_view_point_set(0)[index]\n",
    "\n",
    "    if point_set.shape[0]<2048:\n",
    "            indexes=np.arange(np.shape(point_set)[0])\n",
    "            indexes2=np.random.choice(indexes, 2048-np.shape(indexes)[0],replace=True)\n",
    "            indexes=np.concatenate((indexes,indexes2))\n",
    "    else:\n",
    "            indexes=np.random.choice(np.arange(np.shape(point_set)[0]),2048,replace=False)\n",
    "            \n",
    "    point_set = np.reshape(point_set[indexes], (1, 2048, 4))\n",
    "    for j in range(1000):        \n",
    "        _, x =sess.run([train_op, loss],feed_dict={idx:ind, idx1:ind1, label_out: l.astype(np.int32), label_out1: l1.astype(np.int32), inputs:point_set})\n",
    "        #print(x)\n",
    "        #print(c)\n",
    "        #print(n)\n",
    "    prob = sess.run([sig],feed_dict={idx:ind, idx1:ind1, label_out: l.astype(np.int32), label_out1: l1.astype(np.int32), inputs:point_set})\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(h)>0.5)\n",
    "np.sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(ind, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "# Runs the op.\n",
    "print(sess.run(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(zip(list(s),list(e))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/device:GPU:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "# Creates a session with allow_soft_placement and log_device_placement set\n",
    "# to True.\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=False, log_device_placement=True)) as sess:\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
